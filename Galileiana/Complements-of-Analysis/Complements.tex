\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{nicefrac}
\usepackage[margin=3.3cm]{geometry}
\usepackage{amssymb}
\usepackage{accents}
\usepackage{amsthm}
\usepackage[pdftex, pdfborderstyle={/S/U/W 0}]{hyperref}
\numberwithin{equation}{section}
\usepackage{commath}
\usepackage{graphicx}
%\usepackage[extreme]{savetrees}
\usepackage{bm}
\usepackage{indentfirst}
\usepackage{nicefrac}
\setcounter{tocdepth}{4}
\usepackage{fnpct}
\usepackage{centernot}

\usepackage{cool}
\Style{DSymb={\mathrm d},DShorten=true,IntegrateDifferentialDSymb=\mathrm{d}}

\usepackage{microtype}
\usepackage{cleveref}
\usepackage{tensor}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{mathtools}
 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{url}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}
\author{Jacopo Tissino}
\title{Notes on Complements of Analysis}

\begin{document}

\maketitle

\chapter{Set theory}

\section{The ZFC axioms}

\paragraph{Extensionality}

\begin{equation}
\forall x: \forall y: \forall a: x=y \iff (a \in x \iff a \in y)
\end{equation}

\paragraph{Existence of the null set}

\begin{equation}
\exists x: \forall y: y \notin x
\end{equation}

\paragraph{Foundation}

Every nonempty set contains an $\in$-minimal element:

\begin{equation}
\forall A: \exists x \in A : \forall y \in A: y \notin x
\end{equation}

This means that there cannot be an infinite $\in$ chain like $A_1 \ni A_2 \ni A_3 \ni \dots$.

We can also say that $\forall A: \exists x \in A: x \cap A = \emptyset$.

This also exclude the existence of the set of all sets: $\nexists x: \forall y: y \in x$.

\paragraph{Separation}

Given a well-defined property $P(x)$, there exists a set such that

\begin{equation}
\forall y : \forall x: x \in y \wedge P(x)
\end{equation}

This implies the existence of the empty set, and excludes Russel's paradox.

\paragraph{Pair sets}

\begin{equation}
\forall a: \forall b: \exists x: \forall y: y \in x \iff (y=a \vee y=b)
\end{equation}

This implies the existence of singlets, and of ordered pairs, defined as: $(a, b) := \lbrace \lbrace a\rbrace, \lbrace a, b\rbrace\rbrace$ ($a = \cap (a, b)$, $b = \cup (a, b) \diagdown \cap (a, b)$).

Of course, 

\begin{equation}
(a, b) = (c, d) \iff (a=c) \wedge (b=d)
\end{equation}

\paragraph{Union set axiom}

\begin{equation}
\forall x: \exists u: \forall z: \exists y: z \in u \iff (z \in y \wedge y \in x ) 
\end{equation}

The usual notation is $u = \cup x$, or $A\cup B$. This also enables us to define intersections:

\begin{equation}
A \cap B = \left\lbrace x \in  \left\lbrace A \cup B \right\rbrace : x \in A \wedge x \in B\right\rbrace
\end{equation}

\paragraph{Power set axiom}

\begin{equation}
\forall x: \exists p: \forall y: y \in p \iff y \subseteq x
\end{equation}

The usual notation is: $p = \mathcal{P}(x)$.

\paragraph{Infinity}

\begin{equation}
\exists x: \forall y: \emptyset \in x \wedge (y \in x \implies y \cup \lbrace y \rbrace \in x )
\end{equation}

\paragraph{Replacement}

Given the set $A$, we can construct the set $\lbrace x \in A : R(x)$. This allows us to construct infinite unions: given the sets $A_i$, $i \in \mathbb{N}$,

\begin{equation}
W = \mathbb{N} \rightarrow \lbrace A_0, A_1, A_2 ...\rbrace = I
\end{equation}

then $\exists \cup I = \cup _{n \in \mathbb{N}} A_n$.

\paragraph{Choice}

Given a set $A$ of nonempty sets, such that any two are disjoint, we can always find a set $B$ containing exactly one element for any element of $A$.

(Reformulate as: $A\times B =\emptyset$ iff $A= \emptyset$ or $B =\emptyset$.)

\subsection{Goedel}

In 1938 Goedel proved that ZFC is coherent. In 1931 he proved that any coherent axiom set contains undecidable propositions. One example for ZFC is the continuum hypothesis.

\section{The Von Neumann Integers}

We define $0_{VN} = \emptyset$, and $S(n_{VN}) =n \cup \lbrace n_{VN} \rbrace$. So $1_{VN} = \lbrace \emptyset \rbrace$, $2_{VN} = \lbrace \emptyset , \lbrace \emptyset \rbrace \rbrace$, $3_{VN} = \lbrace \emptyset , \lbrace \emptyset \rbrace , \lbrace \emptyset , \lbrace \emptyset \rbrace \rbrace \rbrace$...

The $\leq$  relation is thus replaced by $\subseteq$, and $<$ by $\in$.

The axiom of infinity seems to define the VN integers, but many sets could have those properties. So, we define the property $P(x) = \emptyset \in x \wedge (y \in x \implies y \cup \lbrace y \rbrace \in x )$

We'd like to intersect all the sets satisfying $P(x)$ (HOW?)

\begin{equation}
\omega:= \lbrace k \in x : k \in Y \iff \forall Y \in \mathcal{P}(x): P(y)
\end{equation}

to get the actual set $\mathbb{N}$.

\section{Cardinality}

Given the sets $A$ and $B$, we say they have the same cardinality if there exists a bijective $f: A \rightarrow B$.

Having the same cardinality is an equivalence relation, but the set of all the sets with the same cardinality is not a set.

We say that $\abs{a} \leq \abs{B}$ if $\exists f : A\rightarrow B$ injective. This is an order relation: it is

\begin{itemize}
\item reflexive: $\abs{A} \leq \abs{A}$;
\item transitive: $\abs{A} \leq \abs{B} \wedge \abs{B} \leq \abs{C} \implies \abs{A} \leq \abs{C}$;
\item antisymmetric: $\abs{A} \leq \abs{B} \wedge \abs{B}\leq \abs{A} \implies \abs{A} = \abs{B}$;\label{card-antisymmetry}
\item connected if there is a good ordering: $\forall A, B : \abs{A} \leq \abs{B} \vee \abs{B}\leq \abs{A}$.
\end{itemize}

Saying that there exists a surjective $g: B\rightarrow A$ is equivalent to saying there exists an injective $f : A\rightarrow B$.

\begin{proof}
If there exists an injective $f : A\rightarrow B$, we define $g(y) := f^{-1} (y)$ if $y \in f(A)$, and any $a_0$ otherwise. $g$ is surjective.

If there exists a surjective $g: B\rightarrow A$, we define $f(x)$ as any element in $g^{-1}\lbrace x\rbrace$. We need the axiom of choice for this.
\end{proof}

\begin{proof}[Proof of point \ref{card-antisymmetry}, by Cantor-Bernstein-Schroeder]
We have the two bijective functions $f : A\rightarrow B$ and $g : B\rightarrow A$. Then 
\end{proof}

\chapter{Inequalities}

\section{Means}

We shall treat sequences ($n$-uples) in the form $a = (a_1, a_2, \dots , a_n)$ where $\forall i: a_i \geq 0$.

\begin{definition}
The $r$-mean of $a$, denoted $M_r (a)$ or just $M_r$, is:

\begin{equation}
M_r (a) := \left( \frac{1}{n} \displaystyle\sum _{i=1}^n a_i^r \right) ^{\frac{1}{r}}
\end{equation}

If $r<0$ and $\exists i : a_i = 0$, we take $M_r = 0$.
\end{definition}

Some notable means are:

\begin{itemize}
\item the arithmetic mean $A := M_1$; any mean can be written as $M_r = A (a_i^r )^{1/r}$
\item the harmonic mean $H := M_{-1}$
\item the geometric mean $G (a ) := \sqrt[n]{\prod_i a_i} = \exp [ A (\log a_i) ]$
\end{itemize}

\begin{definition}
Given the set of weights $p = (p_1, p_2, \dots, p_n)$, where $\forall i: p_i>0$, the weighted sum $M_r$ is:

\begin{equation}
M_r = M (a, p ) :=\left( \frac{\displaystyle \sum_i p_i a_i^r}{\displaystyle\sum_i p_i}\right)^{1/r}
\end{equation}

The weighted geometric mean, from the definition, is

\begin{equation}
G := \left( \prod_i a_i ^{p_i} \right) ^{1/\sum_i p_i}
\end{equation}

\end{definition}

We notice that means are 1-homogeneous, that is, $\forall \lambda \in \mathbb {R}^+: M_r (\lambda a) = \lambda M_r (a)$.

We can always suppose that the weights all add up to 1. If this is true, we call then $q_i$.

We notice that $\min a_i \leq M_r (a) \leq \max a_i$, with equality iff all the $a_i$ are equal, or if $r<0$ and $\exists a_i = 0$. The same is true for the geometric mean.

\begin{theorem}
\begin{equation}
\lim_{r \rightarrow 0 } M_r (a) = G (a)
\end{equation}
\end{theorem}

\begin{proof}

Suppose that $\forall i: a_i > 0$. We look at the log of $M_r$:

\begin{equation}
\lim_{r\rightarrow 0} \log M_r = \lim_{r\rightarrow 0 } \frac{1}{r} \log \sum _i q_i a_i ^r
\end{equation}

Now, $\sum_i q_i a_i^r$ goes to 1 under our hypotheses.

So, adding and subtracting 1 to the sum (since the $q_i$ add up to 1), and multiplying and dividing, we get:

\begin{equation}
\lim_{r\rightarrow 0} \frac{1+ \log\left(\displaystyle\sum_i q_i (a_i^r -1 ) \right)}{\displaystyle\sum_i q_i (a_i^r -1)}\frac{\displaystyle\sum_i q_i ( a_i^r -1 )}{r}
\end{equation}

but by the limit $\lim_{x\rightarrow 0} \log (1+x) /x =1$ the first fraction goes to 1, so we get:

\begin{equation}
\lim_{r\rightarrow 0} \frac{\sum_i q_i (a_i^r)}{r}
\end{equation}

which by the linearity of the limit and the limit $\lim_{x\rightarrow 0} (a^x -1)/x = \log a$ equals:

\begin{equation}
\sum_i q_i \log a_i = \log \left(\prod_i a_i ^{q_i} \right) = \log G (a)
\end{equation}

In the case where $\exists a_i = 0$, we take the sets $b = \lbrace a_i \neq 0 \rbrace$, $s = \lbrace \text{the corresponding } q_i \rbrace$.

Now, in the limit

\begin{equation}
\lim _{r\rightarrow 0^+} M_r (a, q) = \lim_{r\rightarrow 0^+} \left( \sum_i q_i a_i ^r \right) ^{1/r}
\end{equation}

we would like to swap the $a$s for the $b$s and the $q$s for the $s$s, but we need to account for the fact that $\sum_i s_i < 1$. 
So

\begin{equation}
\lim_{r\rightarrow 0^+} \left( \sum_i s_i \right) ^{1/r} M_r (b, s) = 0 = G(a) = \lim_{r\rightarrow 0^-} M_r (a, q)
\end{equation}

by definition. So we define $M_0 := G$.
\end{proof}

\begin{theorem}
\begin{align}
\lim_{r\rightarrow +\infty} M_r (a_i) &= \max (a_i)\\
\lim_{r\rightarrow -\infty} M_r (a_i) &= \min (a_i)
\end{align}
\end{theorem}

\begin{proof}
We take $a_k$ to be the maximum $a_i$. Then, since $(q_i a_i^r) ^{1/r} \leq \left( \sum_i q_i a_i^r\right)^{1/r}$, we can write

\begin{equation}
q_k^{1/r} a_k \leq M_r (a_i) \leq \max (a_i)
\end{equation}

which, by the squeeze theorem, implies the thesis.

For $r\rightarrow -\infty$, we just need to notice that

\begin{equation}
M_{-r} a_i = \frac{1}{M_r \left(\frac{1}{a_i}\right)}
\end{equation}

and that the maximum of the $1/a_i$ corresponds to the minimum of the $a_i$.
\end{proof}

\paragraph{Cauchy's Inequality}

\begin{theorem}
Given two sequences of numbers $a_i$ and $b_i$ with the usual properties:

\begin{equation}
\left( 
    \sum_i a_i b_i 
\right)^2
\leq
\left(
    \sum_i a_i ^2
\right)
\left(
    \sum_i b_i ^2
\right)
\end{equation}

The equality holds iff the vectors $a$ and $b$ are linearly dependent.
\end{theorem}

\begin{proof}
We can rearrange the inequality like:

\begin{align}
\left(
    \sum_i a_i ^2
\right)
\left(
    \sum_j b_j ^2
\right) -
\left( 
    \sum_k a_k b_k 
\right)^2
&\geq 0\\
\left(
    \sum_i a_i ^2
\right)
\left(
    \sum_j b_j ^2
\right) -
\left( 
    \sum_i a_i b_i 
\right)
\left( 
    \sum_j a_j b_j 
\right)
&\geq 0\\
\sum_{i, j} a_i^2 b_j^2 -\sum_{i, j} a_i b_i a_j b_j &\geq 0
\\
\frac 12 \sum_{i, j} 2\left(
a_i^2 b_j^2 - a_i b_i a_j b_j
\right)&\geq 0
\\
\frac 12 \sum_{i, j} \left(
a_i b_j - b_i a_j
\right)^2 &\geq 0
\end{align}

Where, in the last passage, we have swapped some indices which would have been summed over in another iteration anyway. Now, this is clearly true. 

$a$ and $b$ are proportional iff $\forall i, j : a_i b_j - b_i a_j = 0$, that is, the matrix they span has rank 1.
\end{proof}

This implies that $\forall r>0: M_r \leq M_{2r}$, with equality iff all the $a_i$ are equal. This can be easily proven by setting $a_i := \sqrt{p_i}$ and $b_i := \sqrt{p_i} a_i^r$ and applying the theorem.

\begin{theorem}
$G\leq A$.
\end{theorem}

\begin{proof}
$A = M_1 \geq M_{1/2} \geq M_{1/4} \geq M_{1/8}\geq \dots  \geq \lim_{r\rightarrow 0} M_r = G$
\end{proof}

\begin{theorem}[Young's Inequality]
\begin{equation}
\forall a, b \geq 0: \forall p>1 : ab \leq \frac{a^p}{p} + \frac{b^{p'}}{p'}
\end{equation}

where $p^{-1} + p'^{-1} = 1$. Equality holds iff $b = a^{p-1}$.
\end{theorem}

\begin{proof}
We just need to use $G\leq A$ with the set $(a^p, b^{p'})$ and as weights $(p^{-1}, p'^{-1})$.
\end{proof}

\begin{proof}[Integral version of the proof]
Suppose WLOG that $b\leq a^{p-1}$. Now, graph the function $y = x^{p-1}$. Now, in $[0; +\infty] \times [0; +\infty]$, consider the area between $x=a$ and $y=b$, it is $ab$ and surely less than the sum of these integrals:

\begin{equation}
ab \leq \int^{a}_{0} x^{p-1} \, \text{d}x + \int^{b}_{0} y^{\frac{1}{p-1}} \, \text{d} y = \frac{a^p}{p} + \frac{b^{\frac{1}{p-1} +1}}{\frac{1}{p-1} +1} = \frac{a^p}{p} + \frac{b^{p'}}{p'}
\end{equation}
\end{proof}

\paragraph{Hoelder}

\begin{theorem}
Given some $n$-uples, $(a_{ji})$ ($j$ is the index of the tuple number, and $i$ is the element in the tuple) and some weights $\alpha_i$ such that $\sum_i \alpha_i = 1$, the following holds:

\begin{equation}
\sum_i \left( \prod_j a_{ji}^{\alpha_j}\right) \leq \prod_j \left(
\sum _i a_{ji} 
\right)^{\alpha_j}
\end{equation}

with equality iff all the $n$-uples are proportional.
\end{theorem}

\begin{proof}
If one of the tuples is 0 in every position, then the theorem is automatically proven.

Otherwise, we can divide the left side of the inequality by the right to get:

\begin{equation}
\frac{\displaystyle
\sum_i \left(
\prod_j a_{ji}^{\alpha_j}
\right)
}{\displaystyle
\prod_j
\left(
\sum_i a_{ji}
\right)^{\alpha_j}
}
=
\sum_i
\left(
\prod_j
\left(
\frac{
a_{ji}
}{
\sum_k a_{jk}
}
\right)^{\alpha_j}
\right)\leq 1
\end{equation}

but 

\begin{equation}
\sum_i
\left(
\prod_j
\left(
\frac{
a_{ji}
}{
\sum_k a_{jk}
}
\right)^{\alpha_j}
\right)\leq
\sum_i
\left(
\sum_j \alpha_j \left(
\frac{a_{ji}}{\sum_k a_{jk}}
\right)
\right) =
\sum_j \alpha_j
\frac{\sum_i a_{ji}}{\sum_k a_{jk}}
=1
\end{equation}

by $G\leq A$, and since $\sum_k \alpha_k = 1$.
\end{proof}

\chapter{Convex functions}

\section{Definition}

\begin{definition}
Given an interval $I \subseteq \mathbb{R}$, $f: I\rightarrow \mathbb{R}$ is convex if, $\forall x_1, x_2 \in I$ and $\forall \lambda \in [0, 1]$:

\begin{equation}
f(\lambda x_1 + (1-\lambda) x_2) \leq \lambda f(x_1) + (1-\lambda)f(x_2)
\end{equation}

This is read as ``$f$ applied to a convex combination of $x_1$ and $x_2$ is less than or equal to a convex combination of $f(x_1)$ and $f(x_2)$.''
\end{definition}

$x = \lambda x_1 + (1-\lambda) x_2$ is a convex combination (a kind of weighted average) of $x_1$ and $x_2$; we clearly have $x_1 \leq x \leq x_2$, reaching equality on one side or the other for $\lambda = 1$ or $\lambda = 0$ respectively.

An alternative definition is: $f$ is convex if $\forall x_1, x, x_2: x_1 \leq x \leq x_2$:

\begin{equation}
f(x_1) + \frac{f(x_2) - f(x_1)}{x_2 - x_1}(x - x_1) \geq f(x)
\end{equation}

\begin{proof}
\end{proof}

\begin{theorem}
If $f$ is convex, then it is Lipschitz-continuous, which implies it is continuous, at least in the interior of an interval. Also, if the derivative is defined, then it is bounded.
\end{theorem}

\begin{proof}
\end{proof}

The left and right derivatives of a convex function are defined everywhere.
The following hold:

\begin{enumerate}
\item $f'_-(x) \leq f'_+ (x)$;
\item both are non decreasing;
\item $x<y \implies f'_+(x) \leq f'_- (y)$.
\end{enumerate}

\begin{theorem}
\begin{equation}
\forall x_0 \in I: \exists m \in \mathbb{R}: \forall x \in I: f(x) \geq f(x_0) + m (x - x_0)
\end{equation}

Clearly, $m$ is between the left and right derivative.
\end{theorem}

\begin{theorem}
The set of points in which $f: I \rightarrow \mathbb{R}$ is not differentiable (that is, the left and right derivatives are not continuous) is at most countable.
\end{theorem}

\begin{corollary}
If $f$ is differentiable in $I$, then it is convex if $f'$ is increasing, and if it is two times differentiable, it is convex if $f''\geq 0$.
\end{corollary}

The right derivative is right-continuous, the left derivative is left-continuous. The points where $f$ is differentiable coincide with those where $f'_+$ and $f'_-$ are continuous.

\begin{theorem}[Jensen's inequality]
If $F$ is convex, then $\forall g: [0, 1] \rightarrow \mathbb{R}$:
\begin{equation}
\int^{1}_{0} F(g(x)) \, \text{d} x  \geq F\left( \int^{1}_{0} g(x) \, \text{d}x \right)
\end{equation}
\end{theorem}

\chapter{Discrete dynamical systems}

These are systems in which time is quantized, and things change over time. We are given a function, and we analyze its behaviour when we iterate it. An example is the logistic equation:

\begin{equation}
P_{n+1} = \lambda P_n (1- P_n)
\end{equation}

Another one is the algorithm to calculate $\sqrt{5}$: choose any $P_0$ and apply

\begin{equation}
P_{n+1} = \frac{1}{2} \left( P_n + \frac{5}{P_n} \right)
\end{equation}

\section{Basic notation}

The notation we will use for iteration is:

\begin{equation}
f^n (x) =\overbrace{f \circ f \circ \cdots \circ f \circ f}^n (x)
\end{equation}

\begin{definition}
Given a point $x_0 \in \mathbb{R}$ and a function $f: \mathbb{R} \rightarrow \mathbb{R}$, the orbit of $x_0$ is the sequence of the $x_n := f^n (x_0)$ with $n\in \mathbb{N}$.
\end{definition}

\begin{definition}
$x_0$ is a fixed point if $f(x_0) = x_0$.
\end{definition}

\begin{definition}
$x_0$ is periodic with period $k$ if $f^k (x_0) = x_0$. The least $k$ for which this is true is the minimal period.
\end{definition}

\begin{definition}
A point is definitively fixed (or periodic) if in its orbit there is a fixed (or periodic) point.
\end{definition}

\section{Graphical analysis}

\section{Fixed points}

\begin{theorem}
Given the continuous function $f: [a, b] \rightarrow [a, b]$, $f$ has at least one fixed point.
\end{theorem}

\begin{proof}
We can define the auxiliary function $g(x):= f(x) -x$. We then see that $g(a) \geq 0$ and $g(b) \leq 0$, then $\exists c \in [a, b]: g(c) = 0$, so $f(c) = c$.
\end{proof}

\begin{definition}
The fixed point $x_0$ of the function $f$ is said to be:
\begin{enumerate}
\item attractive if $\abs{f'(x_0)} < 1$;
\item neutral if $\abs{f'(x_0)} = 1$;
\item repulsive if $\abs{f'(x_0)} > 1$.
\end{enumerate}
\end{definition}

For example, $x^2$ has two fixed points: 0 is attractive, 1 is repulsive.

The reason for this definition is the following theorem:

\begin{theorem}
If $x_0$ is attractive, then $\exists I(x_0, \delta)$ (an interval) such that $\forall x \in I: f^n(x) \rightarrow x_0$.

If $x_0$ is repulsive, then $\exists I(x_0, \delta)$ such that $\forall x \in I: \exists n \in \mathbb{N}: f^n(x) \notin I$.
\end{theorem}

\begin{proof}[Attractive points]
\begin{equation}
\abs{f'(x_0)} = \lim _{x \rightarrow x_0} \abs{\frac{f(x) - f(x_0)}{x - x_0}}
\end{equation}

If $\abs{f'(x_0)} < 1$, then $\exists \delta > 0 , \lambda > 0$ such that 

\begin{equation}
\forall x \in I(x_0, \delta): \abs{\frac{f(x) - f(x_0)}{x - x_0}}< \lambda < 1
\end{equation}

where we can think of $\lambda$ as $f'(x_0) + \varepsilon$.

Then, if $x \in I$, we can show that the orbit converges to $x_0$:

\begin{equation}
\abs{f(x) - f(x_0)} = \abs{f(x) - x_0} < \lambda \abs{x - x_0} < \lambda \delta < \delta
\end{equation}

So the distance from $x_0$ has diminished:

\begin{equation}
\abs{f^n(x) - x_0} < \lambda^n \abs{x - x_0} < \lambda ^n \delta \rightarrow 0
\end{equation}
\end{proof}

\begin{proof}[Repulsive points]
If $\abs{f'(x_0) > 1}$, $\exists \delta >0, \lambda >0$ such that $\forall x \in I(x_0, \delta)$:

\begin{equation}
\abs{\frac{f(x) - f(x_0)}{x - x_0}} > \lambda > 1
\end{equation}

where we can think of $\lambda$ as being $f'(x_0) - \varepsilon$.
Like before, then, we can write

\begin{equation}
\abs{f^n (x_0) - x_0} > \lambda ^n \abs{x - x_0} \rightarrow + \infty
\end{equation}

so for some $n \in \mathbb{N}$ the point will escape the interval.
\end{proof}

Now, we should point out that while a point will surely escape a repulsive point, there is no guarantee that it will stay outside of it; it might even come back to $x_0$ itself.

\section{Periodic points}

If $x_0$ is $n-$periodic for $f$, then it is fixed for $f^n$.

An $n-$periodic point is said to be attractive or repulsive if it is for $f^n$. Then, we call the orbit of $x_0$ attractive or repulsive.

\begin{theorem}
If $x_n \rightarrow L \in \mathbb{R}$, and $f$ is continuous in $L$, then $L$ is a fixed point for $f$.
\end{theorem}

\begin{proof}
\begin{equation}
L = \lim_{n\rightarrow \infty} x_{n+1} = \lim_{n \rightarrow \infty} f(x_n) = f(L)
\end{equation}
\end{proof}

We can use the same proof for the case in which $x_{kn} \rightarrow L$, with $k \in \mathbb{N}$:

\begin{equation}
L = \lim_{n\rightarrow \infty} x_{k(n+1)} = \lim_{n \rightarrow \infty} f^k(x_{kn}) = f^k(L) 
\end{equation}

\subsection{Chain rule}

To check whether a point is fixed for $f^n$ we need to look at its derivative: we can use the chain rule to get

\begin{equation}
(f^n)' (x_0) = (f^{n-1})' (f(x_0)) f'(x_0) = \prod_{i=0}^{n-1} f'(f^i(x_0)) =\prod_{i=0}^{n-1} f'(x_i)
\end{equation}

Where we use the notation in which $x_{n+1} = f(x_n)$.
We then notice that $\forall i \in \lbrace 1 \dots n \rbrace: (f^n)' (x_0) = (f^n)'(f^i (x_0))$: we can swap the order of the points.



\chapter{Miscellaneous}

\section{Wallis}

\begin{theorem}[Wallis]
\begin{equation}
\lim_{n\rightarrow +\infty} \frac{\displaystyle\prod_{i=1}^{n} (2i)^2}{(2n+1)\displaystyle \prod _{i=1}^{n-1} (2i+1)^2}  = \frac{\pi}{2}
\end{equation}
\end{theorem}

\begin{proof}
We define the succession $I_n$ as:

\begin{equation}
I_n = \int^{\frac{\pi}{2}}_{0} (\sin x )^n \, \text{d}x
\end{equation}

Clearly a first property is $I_{n+1} \leq I_n$. Then, if $n\geq 2$, we can calculate:

\begin{align*}
I_n &= \int^{\frac{\pi}{2}}_{0} \sin^2 (x) \sin^{n-2} x \, \text{d}x\\
&= \int^{\frac{\pi}{2}}_{0} (1-\cos^2 x) \sin^{n-2} x \, \text{d}x\\
&= I_{n-2} -\int^{\frac{\pi}{2}}_{0} (\cos^2 x) \left( \frac{\sin^{n-1} x}{n-1} \right)' \, \text{d}x\\
&= I_{n-2} -\frac{1}{n-1} \left(\left. \cos x \sin^{n-1} x \right]^{\frac{\pi}{2}}_0 + \int^{\frac{\pi}{2}}_{0} \sin^n x \, \text{d}x \right)\\
&= I_{n-2} - \frac{1}{n-1} I_n
\end{align*}

therefore $I_n = I_{n-2} ((n-1)/n)$.

Now, consider:

\begin{equation}
\frac{I_{2n+1}}{I_{2n}} = \frac{2n}{2n+1}\leq \frac{I_2n+1}{I_{2n}} \leq 1
\end{equation}

this, by the squeeze theorem, implies

\begin{equation}
\lim_{n\rightarrow +\infty} \frac{I_{2n+1}}{I_{2n}} =1
\end{equation}

But we can expand the $I_n$ into products of the even or odd $I_n$ preceding them, so we get:

\begin{equation}
\lim_{n\rightarrow+\infty} \frac{I_{2n+1}}{I_{2n}} = \frac{I_1 \displaystyle\prod_{i=1}^{n} \frac{2i}{2i+1}}{I_0 \displaystyle \prod _{i=1}^{n} \frac{2i-1}{2i}} = \frac{\displaystyle\prod_{i=1}^{n} (2i)^2}{(2n+1)\displaystyle \prod _{i=1}^{n-1} (2i+1)^2} \frac{I_1}{I_0}
\end{equation}

and $I_0 = \pi /2$, $I_1 = 1$.
\end{proof}

\section{Stirling}

\begin{theorem}
For large enough numbers, the factorial can be approximated as $n! \sim n^n e^{-n} \sqrt{2\pi n}$; that is:

\begin{equation}
\lim_{n\rightarrow +\infty} \frac{n!}{n^n e^{-n} \sqrt{2\pi n}} = 1
\end{equation}
\end{theorem}

\begin{proof}
We define the succession $a_n$ as:

\begin{equation}
a_n := \log \left(
\frac{n!}{n^n e^{-n} \sqrt{n}}
\right) =
\log n! - \log \left(
\left(
n+ \frac 12
\right) \log n  -n
\right)
\end{equation}

So we just need to prove that $\lim_{n\rightarrow +\infty} a_n = \log (\sqrt{2\pi})$. First, we will show that it is strictly decreasing:

\begin{align}
a_n - a_{n+1} &= \left(
n + \frac 12 \right) \log \left(
\frac{n+1}{n}\right) -1\nonumber \\
&=
(2n+1) \left(
\frac 12 \log \left( \frac{1 + \frac{1}{2n+1}}{1 - \frac{1}{2n+1}}
\right) - \frac{1}{2n +1} \right) \label{term-difference-stirling}
\end{align}

We will now use the fact that, $\forall t \in [0;1]$:

\begin{equation}
\frac 12 \log \left(
\frac{1+t}{1-t}
\right) -t <
\frac{t^3}{3(1-t^2)} \label{taylor-log-stirling}
\end{equation}

this can be proven with derivatives,\footnote{\begin{equation}
\frac{\dif}{\dif t} \left(
\frac 12 \left( \log \left(
\frac{1+t}{1-t}
\right) -t\right) - \frac{t^3}{3(1-t^2)}
\right) =
\frac{-2 t^4 -3 t^2 + 3}{3 (t^2 -1)^2} < 0
\end{equation}

and we have equality for $t=0$ in \eqref{taylor-log-stirling}.} but a deeper reason is the fact that $\forall t \in [-1; 1]$ we can write the following MacLaurin series:

\begin{align}
\log(1+t) &= \sum_{i=1}^{+\infty} \frac{(-1)^{i+1} t^i}{i}\\
\log(1-t) &= - \sum_{i=1}^{+\infty} \frac{t^i}{i}
\end{align}

Then we can expand:

\begin{equation}
\log\left(
\frac{1+t}{1-t}\right)
 = \log (1+t) -\log (1-t) = 2\sum_{i=0}^{+\infty} \frac{t^{2i+1}}{2i+1}
\end{equation}

so

\begin{equation}
\frac 12 \log \left( \frac{1+t}{1-t} \right) <
t + \frac{t^3}{3} \left(
\sum_{i=1}^{+\infty} t^{2i}
\right) = t + \frac{t^3}{3 (1-t^2)}
\end{equation}

Now we can take $t = (2n+1)^{-1}$ in equation \eqref{term-difference-stirling}, to get the following formula for $a_n - a_{n+1}$:

\begin{align}
f(t) = \frac 1t \left(
\frac 12 \log \left(\frac{1+t}{1-t}\right) -t
\right) &<
\frac{t^2}{3(1-t^2)} \\
&=
\frac{(2n+1)^{-2}}{3(1-(2n+1)^{-2}} \\ 
&= \frac 1{12n^2 + 12n} \\ 
&= 12 \left(\frac 1n - \frac{1}{n+1}\right)
\end{align}

So, we have proven that the function $a_n - (12 n) ^{-1}$ is strictly increasing; that is, its limit is either real or $+\infty$. Now, we shall prove that $a_n$ is stricty decreasing, that is, $0<a_n - a_{n+1}$.

It is enough for $f(t)$ to satisfy this condition:

\begin{equation}
f'(t) =\frac 12 \frac{1}{1+t} + \frac{1}{2} \frac{1}{1-t} > 0 \quad \forall t \in [0, 1]
\end{equation}

and $f(0) = 0$, so $\forall t \in [0, 1]: f(t)>0$. So $a_n$ is strictly decreasing, thus its limit is either real or $-\infty$. But the limits of $a_n$ and $a_n - (12n)^{-1}$ must be the same since their difference has limit 0: so $\lim_{n\rightarrow +\infty} a_n = c \in \mathbb{R}$.

This means that

\begin{equation}
\lim_{n\rightarrow +\infty} \frac{n!}{n^n e^{-n} \sqrt{2\pi n}} = e^c
\end{equation}

Now we can apply Wallis:

\begin{align*}
\lim_{n\rightarrow +\infty} \frac{\displaystyle\prod_{i=1}^{n} (2i)^2}{(2n+1)\displaystyle \prod _{i=1}^{n-1} (2i+1)^2}  &= \lim_{n\rightarrow \infty} \frac{(2^n n!)^2}{(2n+1) \left( \frac{(2n)!}{2^n n!}\right)^2}\\
&=\lim_{n\rightarrow \infty} \frac{2^{4n} (n!)^4}{(2n!)^2 (2n+1)} \\
&=\lim_{n\rightarrow \infty} \frac{2^{4n} \left( e^c \frac{n^n}{e^n} \sqrt{n} \right)^4}{\left( e^c \frac{(2n)^{2n}}{e^2n} \sqrt{2n} \right)^2 (2n+1)}\\
&=\lim_{n\rightarrow \infty} (e^c)^2 \frac{n^2}{2n(2n+1)} = \frac{(e^c)^2}{4}
= \frac{\pi}{2} 
\end{align*}
\begin{equation}
e^c = \sqrt{2\pi} \implies c = \log(\sqrt{2\pi})
\end{equation}

\end{proof}

\tableofcontents

\end{document}
