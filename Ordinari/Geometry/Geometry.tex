\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{nicefrac}
\usepackage[margin=3.3cm]{geometry}
\usepackage{amssymb}
\usepackage{accents}
\usepackage{amsthm}
\usepackage[pdftex, pdfborderstyle={/S/U/W 0}]{hyperref}
\numberwithin{equation}{section}
\usepackage{commath}
\usepackage{graphicx}
%\usepackage[extreme]{savetrees}
\usepackage{bm}
\usepackage{indentfirst}
\usepackage{nicefrac}
\setcounter{tocdepth}{4}
\usepackage{fnpct}
\usepackage{centernot}

\usepackage{cool}
\Style{DSymb={\mathrm d},DShorten=true,IntegrateDifferentialDSymb=\mathrm{d}}

\usepackage{microtype}
\usepackage{cleveref}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{mathtools}
 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{url}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}
\author{Jacopo Tissino}
\title{Notes on Geometry and Linear Algebra}

\begin{document}

\maketitle

\chapter{Vectors}

\section{Euclidean space}

We will use the Euclidean $n$-dimentional space as a model for the physical space.
Space is a set, whose elements are points, and whose subsets are lines, planes and hyperplanes, which we will treat algebraically. 
We will need notions of parallelism, measure and orthogonality.

\section{Segments}

An oriented segment (or ``applied vector'') is a subset of a space $S_n$ characterized by an ordered pair of points. It is denoted as such: $P_1 P_2$ is the segment $(P_1, P_2 )$, where $\forall k \in \mathbb{N}: P_k \in S_n$.

There exist ``trivial'' segments $P_1 P_1$.

We define an equivalence relation on $S_n \times S_n$: $\sim$, where $P_1 P_2 \sim Q_1 Q_2 \iff P_1 P_2 Q_2 Q_1$ is a parallelogram.\footnote{It is easy enough to check the three conditions.} Notably, we do not need a notion of distance for this.

We denote the representative as such: $[ ( P_1, P_2 ) ] := \overrightarrow{P_1 P_2} = \mathbf{v}$. These are ``free vectors'', or ``geometrical vectors''.

We have the following operations between them:

\begin{itemize}
\item addition: $\mathbf{a} + \mathbf{b}$
\item multiplication by a scalar: $\alpha \mathbf{a},\quad \alpha\in \mathbb{R}$
\end{itemize}

\paragraph{Addition} If we want to add together two vectors $\overrightarrow{AB}$ and $\overrightarrow{CD}$, first we must represent both as starting from the same point: so we change $\overrightarrow{CD}$ to $\overrightarrow{AE} = \overrightarrow{CD}$. Then, there exists a $K$ such that $\overrightarrow{EK} = \overrightarrow{AB}$, and

\begin{equation}
\overrightarrow{AK} := \overrightarrow{AB} + \overrightarrow{CD}
\end{equation}

\paragraph{Multiplication by a scalar} We use a real number as a scalar because of the continuum hypothesis.\footnote{CH states that there is no set whose cardinality is between that of the integers and that of the reals.}

For any $\alpha \in \mathbb{R}$ and any vector , we define $\alpha \overrightarrow{AB}$ as a vector $\overrightarrow{AC}$ such that $\abs{\overrightarrow{AC}} = \abs{\alpha} \abs{\overrightarrow{AB}}$ and $C$ is on the same side of $A$ as $B$ if $\alpha > 0$ and on the opposite side if $\alpha < 0$.

Defining the zero vector $\mathbf{0} = \overrightarrow{AA}$ we immediately see that:

\begin{itemize}
\item $0 \mathbf{v} = \mathbf{0}$
\item $\alpha \mathbf{0} = \mathbf{0}$
\end{itemize}

\paragraph{Properties of vector operations} The following hold:

\begin{enumerate}
\item addition is commutative: $\mathbf{a} + \mathbf{b} = \mathbf{b} + \mathbf{a}$
\item addition is associative: $\mathbf{a} + (\mathbf{b} + \mathbf{c}) = (\mathbf{a} + \mathbf{b}) + \mathbf{c}$, since they define a parallelepiped
\item there exists a zero vector: $\exists \mathbf{0} : \mathbf{a} + \mathbf{0} = \mathbf{a}$
\item there exists an opposite for every vector: $\exists (-\mathbf{a}) : \mathbf{a} + (\mathbf{-a}) = \mathbf{0}$, also, we notice that $(-\mathbf{a}) = -1 \mathbf{a}$
\item scalar multiplication is distributive: $\alpha (\mathbf{a} + \mathbf{b}) = \alpha \mathbf{a} + \alpha\mathbf{b}$
\item scalar addition distributes as vector addition over scalar multiplication: $(\alpha + \beta) \mathbf{a} = \alpha \mathbf{a} + \beta \mathbf{b}$
\item scalar multiplication is associative: $(\alpha \beta ) \mathbf{a} = \alpha (\beta \mathbf{a})$
\item $1 \mathbf{a} = \mathbf{a}$
\item $0 \mathbf{a} = \mathbf{0}$
\end{enumerate}

\section{Reference frames}

For an $n$-dimensional reference frame we need $n$ vectors (the ``basis vectors'') which are neither parallel to one another nor lying in the same (hyper)plane, so that we can express any vector we want through a linear combination of them; they, however, need not be orthogonal.
We will call our three-dimensional basis vectors $\hat{x}$, $\hat{y}$ and $\hat{z}$. Any vector $\mathbf{p}$ can then be seen as:

\begin{equation}
\mathbf{p} = \alpha \hat{x} + \beta \hat{y} + \gamma \hat{z} = \begin{pmatrix}
\alpha \\ \beta \\ \gamma
\end{pmatrix}
\end{equation}

We can verify through the identities that, as long as we work in a consistent reference system, we can express vector addition and scalar mutiplication through the coordinates as follows:

\begin{equation}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
+
\begin{pmatrix}
y_1 \\ y_2 \\ y_3
\end{pmatrix}
=
\begin{pmatrix}
x_1 + y_1 \\ x_2 + y_2 \\ x_3 + y_3
\end{pmatrix},
\qquad
c
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
=
\begin{pmatrix}
c x_1 \\ c x_2 \\ c x_3
\end{pmatrix}
\end{equation}

\section{Linear dependence}

\paragraph{Parallelism}

To write the fact that $\mathbf{a} \parallel \mathbf{b}$, we could say that $\exists \lambda \in \mathbb{R}: \mathbf{a} = \lambda \mathbf{b}$, but this fails to account for one of the vectors being $\mathbf{0}$. A better formula is this one:

\begin{equation}
\exists \alpha , \beta \in \mathbb{R} : \neg (\alpha = \beta = 0 ):  \alpha \mathbf{a} + \beta \mathbf{b} = \mathbf{0}
\end{equation}

\paragraph{Co-hyper-planar vectors}

With a similar argument, we say that $n$ vectors $\mathbf{x}_i$, where $0<i\leq n$ are in the same $n-1$-plane if

\begin{equation}
\forall i \in \mathbb{N}: 0<i\leq n : \exists \alpha_i \in \mathbb{R}: \exists \alpha_i \neq 0 : \sum_{i=1}^n \alpha_i \mathbf{x}_i = \mathbf{0}
\end{equation}

and we call this a \emph{linear combination} of the vectors $\mathbf{x}_i$.

If there is no such linear combination (with at least one nonzero index) yielding $\mathbf{0}$, the vectors are said to be \emph{linearly independent}; otherwise they are \emph{linearly dependent}.
If one of the vectors is the zero vector, all of them are automatically linearly dependent.

\begin{definition}
A \emph{subspace} is a subset $S$ of the geometrical vector space such that:
\begin{enumerate}
\item $\mathbf{0} \in S$;
\item $\forall \mathbf{a}, \mathbf{b} \in S: \mathbf{a} + \mathbf{b} \in S $;
\item $\forall \mathbf{a} \in S: \forall \lambda \in \mathbb{R}: \lambda \mathbf{a} \in S$;
\end{enumerate}
\end{definition}

In $n$-dimensional space, the linear combinations of up to $n-1$ vectors always generate subspaces.

To represent the subspaces generated by the vectors $\mathbf{v}_i$, we write $\langle v_1; v_2; \dots ; v_n\rangle$. To check whether the number of generators is minimal, we just need to see if they are linearly independent. If they are, they form a \emph{basis} of $S$. The number of basis vectors is called the dimension of $S$.

The canonical basis vectors for $\mathbb{R}^3$ are 

\begin{equation}
\mathbf{e}_1 = \begin{pmatrix}
1\\0\\0
\end{pmatrix} \qquad \mathbf{e}_2 = \begin{pmatrix}
0\\1\\0
\end{pmatrix},\qquad \mathbf{e}_3 = \begin{pmatrix}
0\\0\\1
\end{pmatrix}
\end{equation}

The representation of any $\mathbf{v}$ as a combination of the basis vectors is unique.

\begin{proof}
Suppose there were two sequences of coefficients $x_i$ and $\alpha_i$, both of which represent $\mathbf{v}$. Then,

\begin{equation}
\mathbf{v} = \sum_i x_i \mathbf{e}_i = \sum_i \alpha_i \mathbf{e}_i \implies \mathbf{0} = \sum_i (x_i - \alpha_i) \mathbf{e}_i
\end{equation}

but the basis vectors are linearly independent, so $\forall i: x_i = \alpha_i$.
\end{proof}

To find a basis for a given subspace (expressed with an equation or set of equations) we write the vectors in the subspace with as few coordinates as we can, and then separate the variables.

\section{Scalar product}

Given two vectors $\mathbf{v}$ and $\mathbf{w}$, their scalar product is defined as:

\begin{equation}
\mathbf{v} \cdot \mathbf{w} = \sum_{i} v_i w_i
\end{equation}

Its properties are:

\begin{enumerate}
\item $(\mathbf{u} + \mathbf{v}) \cdot \mathbf{w} = \mathbf{u} \cdot \mathbf{w} + \mathbf{v} \cdot \mathbf{w}$;
\item $\mathbf{v} \cdot \mathbf{w} = \mathbf{w} \cdot \mathbf{v}$;
\item $(\alpha \mathbf{v})\cdot \mathbf{w} = \alpha (\mathbf{v} \cdot \mathbf{w})$;
\item $\mathbf{v}\cdot \mathbf{v} \geq 0$, and $\mathbf{v}\cdot \mathbf{v} \iff \mathbf{v} = \mathbf{0}$
\end{enumerate}

\begin{definition}
We define the \emph{norm} of $\mathbf{v}$ as $\norm{\mathbf{v}} = \sqrt{\mathbf{v}\cdot \mathbf{v}}$.
\end{definition}

Properties:

\begin{enumerate}
\item By definition $\norm{\mathbf{v}}\geq 0$, and $\norm{\mathbf{v}}=0 \iff \mathbf{v} = \mathbf{0}$.
\item $\norm{\alpha \mathbf{v}} = \abs{\alpha} \norm{\mathbf{v}}$;
\item $\norm{\mathbf{v} + \mathbf{w}} \leq \norm{\mathbf{v}} +\norm{\mathbf{w}}$ \label{triangular-ineq}
\end{enumerate}

Property \ref{triangular-ineq} can be proven using the Cauchy-Schwarz inequality:

\begin{theorem}[Cauchy-Schwarz]
$\abs{\mathbf{v}\cdot \mathbf{w}}\leq \norm{\mathbf{v}}\cdot \norm{\mathbf{w}}$
\end{theorem}

\begin{proof}
For a $t\in\mathbb{R}$, we take $(\mathbf{v} + t\mathbf{w})^2 = (\mathbf{v} \cdot \mathbf{v}) + 2t (\mathbf{v}\cdot \mathbf{w}) + t^2 (\mathbf{w} \cdot \mathbf{w})$. By definition, $(\mathbf{v} + t\mathbf{w})^2\geq 0$.

Then, in the variable $t$, it must be that $\Delta < 0$, or

\begin{align}
4 (\mathbf{v} \cdot \mathbf{w})^2 -4 (\mathbf{w}\cdot \mathbf{w})(\mathbf{v}\cdot \mathbf{v}) &< 0\nonumber \\
\mathbf{v}\cdot \mathbf{w} &< \norm{\mathbf{w}} \cdot \norm{\mathbf{v}}\label{cauchy-schwarz}
\end{align}
\end{proof}

\begin{proof}[Proof of property \ref{triangular-ineq}]
Take $(\norm{\mathbf{v}+\mathbf{w}})^2 = (\mathbf{v} +\mathbf{w}) \cdot (\mathbf{v} +\mathbf{w})$. This is equal to $\norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 + 2\, \mathbf{v}\cdot \mathbf{w}$ which, by \eqref{cauchy-schwarz}, is less than or equal to $\norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 + 2 \norm{\mathbf{v}}\norm{\mathbf{w}} = (\,\norm{\mathbf{v}}+\norm{\mathbf{w}})^2$
\end{proof}

Since

\begin{equation}
\forall \mathbf{v}, \mathbf{w} \neq \mathbf{0}: \quad -1 \leq \frac{\mathbf{v} \cdot \mathbf{w}}{\norm{\mathbf{v}}\norm{\mathbf{w}}}\leq 1
\end{equation}

there exists a $\theta\in [0;\pi]$ such that $\cos \theta = (\mathbf{v} \cdot \mathbf{w})/(\norm{\mathbf{v}}\norm{\mathbf{w}})$. We will show that $\theta$ is the angle between the vectors.

\begin{proof}
Take $h$ to be the height (relative to the side $\mathbf{v}$) of the triangle defined by $\mathbf{v}$ and $\mathbf{w}$, and $l$ to be $\norm{\mathbf{v}}$ minus the projection of $\mathbf{w}$ on $\mathbf{v}$.

Then $h = \norm{\mathbf{w}}\sin\theta$ and $\norm{\mathbf{v}}-l = \norm{\mathbf{w}}\cos \theta$. So

\begin{equation}
h^2 + l^2 = (\norm{\mathbf{v} - \mathbf{w}})^2 = \norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 -2 \norm{\mathbf{v}} \norm{\mathbf{w}} \cos \theta
\end{equation}

but it is also the case that

\begin{equation}
(\norm{\mathbf{v} - \mathbf{w}})^2 = \norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 -2 (\mathbf{v}\cdot \mathbf{w})
\end{equation}
\end{proof}

\subsubsection{Angle between vectors}

\begin{equation}
\frac{\mathbf{v}\cdot \mathbf{w}}{\norm{\mathbf{v}}\norm{\mathbf{w}}} = \cos (\theta) \label{cos-angle-scalarproduct}
\end{equation}

where $\theta$ is the angle between the vectors, always taken to be positive.

\paragraph{Orthogonality}

From this follows that $\mathbf{v} \cdot \mathbf{w} = 0 \iff \mathbf{v}\perp \mathbf{w}$.

\section{Vector product}

It is defined as an operation on two vectors in $\mathbb{R}^3$, with the informal determinant

\begin{equation}
\mathbf{v} \times \mathbf{w} =\det \begin{pmatrix}
\hat{i} & \hat{j} & \hat{k}\\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3 
\end{pmatrix}
\end{equation}

This weird way of writing comes from the fact that, if we define the following function:

\begin{equation}
f\begin{pmatrix}
x\\y\\z
\end{pmatrix}
=\det \begin{pmatrix}
x & v_1 & w_1\\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{pmatrix}
\end{equation}

we can check that it is linear, thus there is some vector $\mathbf{p}$ for which

\begin{equation}
\mathbf{p} \cdot \begin{pmatrix}
x\\y\\z
\end{pmatrix}=
\det \begin{pmatrix}
x & v_1 & w_1\\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{pmatrix}
\end{equation}

and so it becomes clear that, in fact, $\mathbf{p} = \mathbf{v}\times \mathbf{w}$ is the vector we can scalar-multiply $\mathbf{x}$ by to get the volume of the parallelepiped defined by $\mathbf{v}$, $\mathbf{w}$ and $\mathbf{x}$.

The properties of the vector product are:

\begin{enumerate}
\item $\mathbf{v} \times \mathbf{w} = -\mathbf{w} \times \mathbf{v}$;
\item $(\mathbf{u} + \mathbf{v}) \times \mathbf{w} = \mathbf{u} \times \mathbf{w} + \mathbf{v} \times \mathbf{w}$;
\item $(\alpha \mathbf{v}) \times \mathbf{w} = \alpha (\mathbf{v}\times \mathbf{w}) = \mathbf{v} \times (\alpha \mathbf{w})$;
\item $\mathbf{v} \times \mathbf{w} \iff \mathbf{v}\parallel \mathbf{w}$;\label{parallelismo-vettoriale}
\item $\mathbf{v} \cdot (\mathbf{v} \times \mathbf{w}) = \mathbf{w} \cdot (\mathbf{v} \times \mathbf{w})$;\label{misto-stessovettore-vettoriale}
\item $\norm{\mathbf{v} \times \mathbf{w}}^2 = \norm{\mathbf{v}}^2\norm{\mathbf{w}}^2 - (\mathbf{v}\cdot \mathbf{w})^2$ (Lagrange's identity);\label{lagrange}
\end{enumerate}

The first three are trivial, although the calculations get cumbersome.

\begin{proof}[Proof of point \ref{parallelismo-vettoriale}]
Suppose that $\mathbf{v} \neq \mathbf{0}$ (otherwise the proposition would be automatically true). So WLOG suppose that $v_1 \neq 0$. Then $x_1 y_3 = x_3 y_1$, and we can divide by $x_1$:

\begin{equation}
w_3 = \left( \frac{w_1}{v_1} \right) w_3
\end{equation}

And also

\begin{equation}
w_2 = \left( \frac{w_1}{v_1} \right) v_2 \quad \text{and} \quad w_1 = \left( \frac{w_1}{v_1} \right) v_1
\end{equation}

So $\mathbf{w}$ is a multiple of $\mathbf{v}$.
\end{proof}

The proofs of points \ref{misto-stessovettore-vettoriale} and \ref{lagrange} follow immediately from expanding the calculations.

A neat consequence of point \ref{lagrange} is the fact that, dividing everything by the square norms, and remembering equation \eqref{cos-angle-scalarproduct} we get:

\begin{equation}
\frac{\norm{\mathbf{v} \times \mathbf{w}}^2}{\norm{\mathbf{v}}^2\norm{\mathbf{w}}^2} + \cos^2 (\theta) = 1
\implies
\frac{\norm{\mathbf{v} \times \mathbf{w}}}{\norm{\mathbf{v}}\norm{\mathbf{w}}} = \sin(\theta)
\end{equation}

where $\theta \in [ 0; \pi [$. So the norm of the vector product is the area of the parallelogram.

\section{Mixed product}

Given three vectors $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathbb{R}^3$, the mixed product is defined as $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) \in \mathbb{R}$.

We can prove that $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = 0 \iff $ the three vectors are linearly dependent, i. e. $\mathbf{u} \in \langle\mathbf{v}; \mathbf{w}\rangle$.

\begin{proof}
First we show the leftward implication: $(\alpha \mathbf{v} + \beta \mathbf{w}) \cdot (\mathbf{v}\times \mathbf{w}) = 0$.

Then, for the rightward implication: we use the fact that if $\mathbf{v}$ and $\mathbf{w}$ are linearly independent then $\forall \mathbf{a} \in \mathbb{R}^3$, we can write $\mathbf{a}$ as a linear combination of $\mathbf{v}, \mathbf{w}$ and $\mathbf{v}\times \mathbf{w}$,\footnote{This will be proved later.} which are linearly independent.
\footnote{\begin{proof}
We will prove this by showing that the only linear combination of these three vectors yielding $\mathbf{0}$ is one where the three coefficients are all zero: take
\begin{equation}
\alpha \mathbf{v}+ \beta \mathbf{w} + \gamma (\mathbf{v} \times \mathbf{w}) = 0
\end{equation}

and scalar-multiply everything by $\mathbf{v} \times \mathbf{w}$. We get $\gamma \norm{\mathbf{v} \times \mathbf{w}}^2 = 0$, so $\gamma = 0$.

Then it must be that $\alpha = \beta = 0$.
\end{proof}}

Then $\mathbf{u} = a \mathbf{v} + b\mathbf{w} + c (\mathbf{v} \times \mathbf{w})$. 

If we scalar-multiply everything by $\mathbf{v} \times \mathbf{w}$ we get:

\begin{equation}
\mathbf{u} \cdot (\mathbf{v}\times\mathbf{w}) = c\, \norm{\mathbf{v}\times \mathbf{w}}^2 \implies c=0
\end{equation}

So $\mathbf{u} = a \mathbf{v} + b \mathbf{w}$.

\end{proof}

The mixed product of three vectors is the volume of the parallelepiped they define:

\begin{equation}
\mathbf{u} \cdot (\mathbf{v}\times\mathbf{w}) =
\norm{\mathbf{u}} \norm{\mathbf{v} \times \mathbf{w}}\cos \alpha
=\det \begin{pmatrix}
u_1 & v_1 & w_1 \\
u_2 & v_2 & w_2 \\
u_3 & v_3 & w_3
\end{pmatrix}
\end{equation}

where $\alpha$ is the angle between $\mathbf{u}$ and $\mathbf{v}\times\mathbf{w}$.

\section{Perpendicular set}

Given some vectors $\mathbf{v}_i$, we define the perpendicular set as

\begin{equation}
\lbrace \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\rbrace ^\perp := \left\lbrace \begin{pmatrix}
x\\y\\z
\end{pmatrix}\in \mathbb{R}^3: \forall i: \begin{pmatrix}
x\\y\\z
\end{pmatrix} \cdot \mathbf{v}_i =0\right\rbrace
\end{equation}

$S^\perp$ is always a subspace:

\begin{proof}
\begin{enumerate}
\item $\mathbf{0} \in S^\perp$;
\item $\mathbf{w}_1 \in S^\perp, \mathbf{w}_2 \in S^\perp \implies \forall i: (\mathbf{w}_1 + \mathbf{w}_2)\cdot \mathbf{v}_i = 0$ because of the distributive property;
\item $\forall i: (\alpha \mathbf{w}_1) \cdot \mathbf{v}_i = \alpha (\mathbf{w}_1 \cdot \mathbf{v}_i) = 0$.
\end{enumerate}
\end{proof}

In $\mathbb{R}^n$, the perpendicular set of a subspace of dimension $d$ has dimension $n-d$.

\tableofcontents

\end{document}
