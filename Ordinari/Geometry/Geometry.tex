\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{nicefrac}
\usepackage[margin=3.3cm]{geometry}
\usepackage{amssymb}
\usepackage{accents}
\usepackage{amsthm}
\usepackage[pdftex, pdfborderstyle={/S/U/W 0}]{hyperref}
\numberwithin{equation}{section}
\usepackage{commath}
\usepackage{graphicx}
%\usepackage[extreme]{savetrees}
\usepackage{bm}
\usepackage{indentfirst}
\usepackage{nicefrac}
\setcounter{tocdepth}{4}
\usepackage{fnpct}
\usepackage{centernot}

\usepackage{cool}
\Style{DSymb={\mathrm d},DShorten=true,IntegrateDifferentialDSymb=\mathrm{d}}

\usepackage{microtype}
\usepackage{cleveref}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\usepackage{mathtools}
 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}

\usepackage{url}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}
\author{Jacopo Tissino}
\title{Notes on Geometry and Linear Algebra}

\begin{document}

\maketitle

\chapter{Vectors}

\section{Euclidean space}

We will use the Euclidean $n$-dimentional space as a model for the physical space.
Space is a set, whose elements are points, and whose subsets are lines, planes and hyperplanes, which we will treat algebraically. 
We will need notions of parallelism, measure and orthogonality.

\section{Segments}

An oriented segment (or ``applied vector'') is a subset of a space $S_n$ characterized by an ordered pair of points. It is denoted as such: $P_1 P_2$ is the segment $(P_1, P_2 )$, where $\forall k \in \mathbb{N}: P_k \in S_n$.

There exist ``trivial'' segments $P_1 P_1$.

We define an equivalence relation on $S_n \times S_n$: $\sim$, where $P_1 P_2 \sim Q_1 Q_2 \iff P_1 P_2 Q_2 Q_1$ is a parallelogram.\footnote{It is easy enough to check the three conditions.} Notably, we do not need a notion of distance for this.

We denote the representative as such: $[ ( P_1, P_2 ) ] := \overrightarrow{P_1 P_2} = \mathbf{v}$. These are ``free vectors'', or ``geometrical vectors''.

We have the following operations between them:

\begin{itemize}
\item addition: $\mathbf{a} + \mathbf{b}$
\item multiplication by a scalar: $\alpha \mathbf{a},\quad \alpha\in \mathbb{R}$
\end{itemize}

\paragraph{Addition} If we want to add together two vectors $\overrightarrow{AB}$ and $\overrightarrow{CD}$, first we must represent both as starting from the same point: so we change $\overrightarrow{CD}$ to $\overrightarrow{AE} = \overrightarrow{CD}$. Then, there exists a $K$ such that $\overrightarrow{EK} = \overrightarrow{AB}$, and

\begin{equation}
\overrightarrow{AK} := \overrightarrow{AB} + \overrightarrow{CD}
\end{equation}

\paragraph{Multiplication by a scalar} We use a real number as a scalar because of the continuum hypothesis.\footnote{CH states that there is no set whose cardinality is between that of the integers and that of the reals.}

For any $\alpha \in \mathbb{R}$ and any vector , we define $\alpha \overrightarrow{AB}$ as a vector $\overrightarrow{AC}$ such that $\abs{\overrightarrow{AC}} = \abs{\alpha} \abs{\overrightarrow{AB}}$ and $C$ is on the same side of $A$ as $B$ if $\alpha > 0$ and on the opposite side if $\alpha < 0$.

Defining the zero vector $\mathbf{0} = \overrightarrow{AA}$ we immediately see that:

\begin{itemize}
\item $0 \mathbf{v} = \mathbf{0}$
\item $\alpha \mathbf{0} = \mathbf{0}$
\end{itemize}

\paragraph{Properties of vector operations} The following hold:

\begin{enumerate}
\item addition is commutative: $\mathbf{a} + \mathbf{b} = \mathbf{b} + \mathbf{a}$
\item addition is associative: $\mathbf{a} + (\mathbf{b} + \mathbf{c}) = (\mathbf{a} + \mathbf{b}) + \mathbf{c}$, since they define a parallelepiped
\item there exists a zero vector: $\exists \mathbf{0} : \mathbf{a} + \mathbf{0} = \mathbf{a}$
\item there exists an opposite for every vector: $\exists (-\mathbf{a}) : \mathbf{a} + (\mathbf{-a}) = \mathbf{0}$, also, we notice that $(-\mathbf{a}) = -1 \mathbf{a}$
\item scalar multiplication is distributive: $\alpha (\mathbf{a} + \mathbf{b}) = \alpha \mathbf{a} + \alpha\mathbf{b}$
\item scalar addition distributes as vector addition over scalar multiplication: $(\alpha + \beta) \mathbf{a} = \alpha \mathbf{a} + \beta \mathbf{b}$
\item scalar multiplication is associative: $(\alpha \beta ) \mathbf{a} = \alpha (\beta \mathbf{a})$
\item $1 \mathbf{a} = \mathbf{a}$
\item $0 \mathbf{a} = \mathbf{0}$
\end{enumerate}

\section{Reference frames}

For an $n$-dimensional reference frame we need $n$ vectors (the ``basis vectors'') which are neither parallel to one another nor lying in the same (hyper)plane, so that we can express any vector we want through a linear combination of them; they, however, need not be orthogonal.
We will call our three-dimensional basis vectors $\hat{x}$, $\hat{y}$ and $\hat{z}$. Any vector $\mathbf{p}$ can then be seen as:

\begin{equation}
\mathbf{p} = \alpha \hat{x} + \beta \hat{y} + \gamma \hat{z} = \begin{pmatrix}
\alpha \\ \beta \\ \gamma
\end{pmatrix}
\end{equation}

We can verify through the identities that, as long as we work in a consistent reference system, we can express vector addition and scalar mutiplication through the coordinates as follows:

\begin{equation}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
+
\begin{pmatrix}
y_1 \\ y_2 \\ y_3
\end{pmatrix}
=
\begin{pmatrix}
x_1 + y_1 \\ x_2 + y_2 \\ x_3 + y_3
\end{pmatrix},
\qquad
c
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
=
\begin{pmatrix}
c x_1 \\ c x_2 \\ c x_3
\end{pmatrix}
\end{equation}

\section{Linear dependence}

\paragraph{Parallelism}

To write the fact that $\mathbf{a} \parallel \mathbf{b}$, we could say that $\exists \lambda \in \mathbb{R}: \mathbf{a} = \lambda \mathbf{b}$, but this fails to account for one of the vectors being $\mathbf{0}$. A better formula is this one:

\begin{equation}
\exists \alpha , \beta \in \mathbb{R} : \neg (\alpha = \beta = 0 ):  \alpha \mathbf{a} + \beta \mathbf{b} = \mathbf{0}
\end{equation}

\paragraph{Linearly dependent vectors}

With a similar argument, we say that $n$ vectors $\mathbf{x}_i$, where $0<i\leq n$ are in the same $n-1$-plane if

\begin{equation}
\forall i \in \mathbb{N}: 0<i\leq n : \exists \alpha_i \in \mathbb{R}: \exists \alpha_i \neq 0 : \sum_{i=1}^n \alpha_i \mathbf{x}_i = \mathbf{0}
\end{equation}

and we call this a \emph{linear combination} of the vectors $\mathbf{x}_i$.

One vector is linearly dependend only if it is $\mathbf{0}$.

If there is no such linear combination (with at least one nonzero index) yielding $\mathbf{0}$, the vectors are said to be \emph{linearly independent}; otherwise they are \emph{linearly dependent}.
If one of the vectors is the zero vector, all of them are automatically linearly dependent.

\begin{definition}
A \emph{subspace} is a subset $S$ of the geometrical vector space such that:
\begin{enumerate}
\item $\mathbf{0} \in S$;
\item $\forall \mathbf{a}, \mathbf{b} \in S: \mathbf{a} + \mathbf{b} \in S $;
\item $\forall \mathbf{a} \in S: \forall \lambda \in \mathbb{R}: \lambda \mathbf{a} \in S$;
\end{enumerate}
\end{definition}

In $n$-dimensional space, the linear combinations of up to $n-1$ vectors always generate subspaces.

To represent the subspaces generated by the vectors $\mathbf{v}_i$, we write $\langle v_1; v_2; \dots ; v_n\rangle$. To check whether the number of generators is minimal, we just need to see if they are linearly independent. If they are, they form a \emph{basis} of $S$. The number of basis vectors is called the dimension of $S$.

The canonical basis vectors for $\mathbb{R}^3$ are 

\begin{equation}
\mathbf{e}_1 = \begin{pmatrix}
1\\0\\0
\end{pmatrix} \qquad \mathbf{e}_2 = \begin{pmatrix}
0\\1\\0
\end{pmatrix},\qquad \mathbf{e}_3 = \begin{pmatrix}
0\\0\\1
\end{pmatrix}
\end{equation}

The representation of any $\mathbf{v}$ as a combination of the basis vectors is unique.

\begin{proof}
Suppose there were two sequences of coefficients $x_i$ and $\alpha_i$, both of which represent $\mathbf{v}$. Then,

\begin{equation}
\mathbf{v} = \sum_i x_i \mathbf{e}_i = \sum_i \alpha_i \mathbf{e}_i \implies \mathbf{0} = \sum_i (x_i - \alpha_i) \mathbf{e}_i
\end{equation}

but the basis vectors are linearly independent, so $\forall i: x_i = \alpha_i$.
\end{proof}

To find a basis for a given subspace (expressed with an equation or set of equations) we write the vectors in the subspace with as few coordinates as we can, and then separate the variables.

\section{Scalar product}

Given two vectors $\mathbf{v}$ and $\mathbf{w}$, their scalar product is defined as:

\begin{equation}
\mathbf{v} \cdot \mathbf{w} = \sum_{i} v_i w_i
\end{equation}

Its properties are:

\begin{enumerate}
\item $(\mathbf{u} + \mathbf{v}) \cdot \mathbf{w} = \mathbf{u} \cdot \mathbf{w} + \mathbf{v} \cdot \mathbf{w}$;
\item $\mathbf{v} \cdot \mathbf{w} = \mathbf{w} \cdot \mathbf{v}$;
\item $(\alpha \mathbf{v})\cdot \mathbf{w} = \alpha (\mathbf{v} \cdot \mathbf{w})$;
\item $\mathbf{v}\cdot \mathbf{v} \geq 0$, and $\mathbf{v}\cdot \mathbf{v} \iff \mathbf{v} = \mathbf{0}$
\end{enumerate}

\begin{definition}
We define the \emph{norm} of $\mathbf{v}$ as $\norm{\mathbf{v}} = \sqrt{\mathbf{v}\cdot \mathbf{v}}$.
\end{definition}

Properties:

\begin{enumerate}
\item By definition $\norm{\mathbf{v}}\geq 0$, and $\norm{\mathbf{v}}=0 \iff \mathbf{v} = \mathbf{0}$.
\item $\norm{\alpha \mathbf{v}} = \abs{\alpha} \norm{\mathbf{v}}$;
\item $\norm{\mathbf{v} + \mathbf{w}} \leq \norm{\mathbf{v}} +\norm{\mathbf{w}}$ \label{triangular-ineq}
\end{enumerate}

Property \ref{triangular-ineq} can be proven using the Cauchy-Schwarz inequality:

\begin{theorem}[Cauchy-Schwarz]
$\abs{\mathbf{v}\cdot \mathbf{w}}\leq \norm{\mathbf{v}}\cdot \norm{\mathbf{w}}$
\end{theorem}

\begin{proof}
For a $t\in\mathbb{R}$, we take $(\mathbf{v} + t\mathbf{w})^2 = (\mathbf{v} \cdot \mathbf{v}) + 2t (\mathbf{v}\cdot \mathbf{w}) + t^2 (\mathbf{w} \cdot \mathbf{w})$. By definition, $(\mathbf{v} + t\mathbf{w})^2\geq 0$.

Then, in the variable $t$, it must be that $\Delta < 0$, or

\begin{align}
4 (\mathbf{v} \cdot \mathbf{w})^2 -4 (\mathbf{w}\cdot \mathbf{w})(\mathbf{v}\cdot \mathbf{v}) &< 0\nonumber \\
\mathbf{v}\cdot \mathbf{w} &< \norm{\mathbf{w}} \cdot \norm{\mathbf{v}}\label{cauchy-schwarz}
\end{align}
\end{proof}

\begin{proof}[Proof of property \ref{triangular-ineq}]
Take $(\norm{\mathbf{v}+\mathbf{w}})^2 = (\mathbf{v} +\mathbf{w}) \cdot (\mathbf{v} +\mathbf{w})$. This is equal to $\norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 + 2\, \mathbf{v}\cdot \mathbf{w}$ which, by \eqref{cauchy-schwarz}, is less than or equal to $\norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 + 2 \norm{\mathbf{v}}\norm{\mathbf{w}} = (\,\norm{\mathbf{v}}+\norm{\mathbf{w}})^2$
\end{proof}

Since

\begin{equation}
\forall \mathbf{v}, \mathbf{w} \neq \mathbf{0}: \quad -1 \leq \frac{\mathbf{v} \cdot \mathbf{w}}{\norm{\mathbf{v}}\norm{\mathbf{w}}}\leq 1
\end{equation}

there exists a $\theta\in [0;\pi]$ such that $\cos \theta = (\mathbf{v} \cdot \mathbf{w})/(\norm{\mathbf{v}}\norm{\mathbf{w}})$. We will show that $\theta$ is the angle between the vectors.

\begin{proof}
Take $h$ to be the height (relative to the side $\mathbf{v}$) of the triangle defined by $\mathbf{v}$ and $\mathbf{w}$, and $l$ to be $\norm{\mathbf{v}}$ minus the projection of $\mathbf{w}$ on $\mathbf{v}$.

Then $h = \norm{\mathbf{w}}\sin\theta$ and $\norm{\mathbf{v}}-l = \norm{\mathbf{w}}\cos \theta$. So

\begin{equation}
h^2 + l^2 = (\norm{\mathbf{v} - \mathbf{w}})^2 = \norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 -2 \norm{\mathbf{v}} \norm{\mathbf{w}} \cos \theta
\end{equation}

but it is also the case that

\begin{equation}
(\norm{\mathbf{v} - \mathbf{w}})^2 = \norm{\mathbf{v}}^2 + \norm{\mathbf{w}}^2 -2 (\mathbf{v}\cdot \mathbf{w})
\end{equation}
\end{proof}

\subsubsection{Angle between vectors}

\begin{equation}
\frac{\mathbf{v}\cdot \mathbf{w}}{\norm{\mathbf{v}}\norm{\mathbf{w}}} = \cos (\theta) \label{cos-angle-scalarproduct}
\end{equation}

where $\theta$ is the angle between the vectors, always taken to be positive.

\paragraph{Orthogonality}

From this follows that $\mathbf{v} \cdot \mathbf{w} = 0 \iff \mathbf{v}\perp \mathbf{w}$.

\section{Vector product}

It is defined as an operation on two vectors in $\mathbb{R}^3$, with the informal determinant

\begin{equation}
\mathbf{v} \times \mathbf{w} =\det \begin{pmatrix}
\hat{i} & \hat{j} & \hat{k}\\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3 
\end{pmatrix}
\end{equation}

This weird way of writing comes from the fact that, if we define the following function:

\begin{equation}
f\begin{pmatrix}
x\\y\\z
\end{pmatrix}
=\det \begin{pmatrix}
x & v_1 & w_1\\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{pmatrix}
\end{equation}

we can check that it is linear, thus there is some vector $\mathbf{p}$ for which

\begin{equation}
\mathbf{p} \cdot \begin{pmatrix}
x\\y\\z
\end{pmatrix}=
\det \begin{pmatrix}
x & v_1 & w_1\\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{pmatrix}
\end{equation}

and so it becomes clear that, in fact, $\mathbf{p} = \mathbf{v}\times \mathbf{w}$ is the vector we can scalar-multiply $\mathbf{x}$ by to get the volume of the parallelepiped defined by $\mathbf{v}$, $\mathbf{w}$ and $\mathbf{x}$.

The properties of the vector product are:

\begin{enumerate}
\item $\mathbf{v} \times \mathbf{w} = -\mathbf{w} \times \mathbf{v}$;
\item $(\mathbf{u} + \mathbf{v}) \times \mathbf{w} = \mathbf{u} \times \mathbf{w} + \mathbf{v} \times \mathbf{w}$;
\item $(\alpha \mathbf{v}) \times \mathbf{w} = \alpha (\mathbf{v}\times \mathbf{w}) = \mathbf{v} \times (\alpha \mathbf{w})$;
\item $\mathbf{v} \times \mathbf{w} \iff \mathbf{v}\parallel \mathbf{w}$;\label{parallelismo-vettoriale}
\item $\mathbf{v} \cdot (\mathbf{v} \times \mathbf{w}) = \mathbf{w} \cdot (\mathbf{v} \times \mathbf{w})$;\label{misto-stessovettore-vettoriale}
\item $\norm{\mathbf{v} \times \mathbf{w}}^2 = \norm{\mathbf{v}}^2\norm{\mathbf{w}}^2 - (\mathbf{v}\cdot \mathbf{w})^2$ (Lagrange's identity);\label{lagrange}
\end{enumerate}

The first three are trivial, although the calculations get cumbersome.

\begin{proof}[Proof of point \ref{parallelismo-vettoriale}]
Suppose that $\mathbf{v} \neq \mathbf{0}$ (otherwise the proposition would be automatically true). So WLOG suppose that $v_1 \neq 0$. Then $x_1 y_3 = x_3 y_1$, and we can divide by $x_1$:

\begin{equation}
w_3 = \left( \frac{w_1}{v_1} \right) w_3
\end{equation}

And also

\begin{equation}
w_2 = \left( \frac{w_1}{v_1} \right) v_2 \quad \text{and} \quad w_1 = \left( \frac{w_1}{v_1} \right) v_1
\end{equation}

So $\mathbf{w}$ is a multiple of $\mathbf{v}$.
\end{proof}

The proofs of points \ref{misto-stessovettore-vettoriale} and \ref{lagrange} follow immediately from expanding the calculations.

A neat consequence of point \ref{lagrange} is the fact that, dividing everything by the square norms, and remembering equation \eqref{cos-angle-scalarproduct} we get:

\begin{equation}
\frac{\norm{\mathbf{v} \times \mathbf{w}}^2}{\norm{\mathbf{v}}^2\norm{\mathbf{w}}^2} + \cos^2 (\theta) = 1
\implies
\frac{\norm{\mathbf{v} \times \mathbf{w}}}{\norm{\mathbf{v}}\norm{\mathbf{w}}} = \sin(\theta)
\end{equation}

where $\theta \in [ 0; \pi [$. So the norm of the vector product is the area of the parallelogram.

\section{Mixed product}

Given three vectors $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathbb{R}^3$, the mixed product is defined as $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) \in \mathbb{R}$.

We can prove that $\mathbf{u} \cdot (\mathbf{v} \times \mathbf{w}) = 0 \iff $ the three vectors are linearly dependent, i. e. $\mathbf{u} \in \langle\mathbf{v}; \mathbf{w}\rangle$.

\begin{proof}
First we show the leftward implication: $(\alpha \mathbf{v} + \beta \mathbf{w}) \cdot (\mathbf{v}\times \mathbf{w}) = 0$.

Then, for the rightward implication: we use the fact that if $\mathbf{v}$ and $\mathbf{w}$ are linearly independent then $\forall \mathbf{a} \in \mathbb{R}^3$, we can write $\mathbf{a}$ as a linear combination of $\mathbf{v}, \mathbf{w}$ and $\mathbf{v}\times \mathbf{w}$,\footnote{This will be proved later.} which are linearly independent.
\footnote{\begin{proof}
We will prove this by showing that the only linear combination of these three vectors yielding $\mathbf{0}$ is one where the three coefficients are all zero: take
\begin{equation}
\alpha \mathbf{v}+ \beta \mathbf{w} + \gamma (\mathbf{v} \times \mathbf{w}) = 0
\end{equation}

and scalar-multiply everything by $\mathbf{v} \times \mathbf{w}$. We get $\gamma \norm{\mathbf{v} \times \mathbf{w}}^2 = 0$, so $\gamma = 0$.

Then it must be that $\alpha = \beta = 0$.
\end{proof}}

Then $\mathbf{u} = a \mathbf{v} + b\mathbf{w} + c (\mathbf{v} \times \mathbf{w})$. 

If we scalar-multiply everything by $\mathbf{v} \times \mathbf{w}$ we get:

\begin{equation}
\mathbf{u} \cdot (\mathbf{v}\times\mathbf{w}) = c\, \norm{\mathbf{v}\times \mathbf{w}}^2 \implies c=0
\end{equation}

So $\mathbf{u} = a \mathbf{v} + b \mathbf{w}$.

\end{proof}

The mixed product of three vectors is the volume of the parallelepiped they define:

\begin{equation}
\mathbf{u} \cdot (\mathbf{v}\times\mathbf{w}) =
\norm{\mathbf{u}} \norm{\mathbf{v} \times \mathbf{w}}\cos \alpha
=\det \begin{pmatrix}
u_1 & v_1 & w_1 \\
u_2 & v_2 & w_2 \\
u_3 & v_3 & w_3
\end{pmatrix}
\end{equation}

where $\alpha$ is the angle between $\mathbf{u}$ and $\mathbf{v}\times\mathbf{w}$.

\section{Perpendicular set}

Given some vectors $\mathbf{v}_i$, we define the perpendicular set as

\begin{equation}
\lbrace \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\rbrace ^\perp := \left\lbrace \begin{pmatrix}
x\\y\\z
\end{pmatrix}\in \mathbb{R}^3: \forall i: \begin{pmatrix}
x\\y\\z
\end{pmatrix} \cdot \mathbf{v}_i =0\right\rbrace
\end{equation}

$S^\perp$ is always a subspace:

\begin{proof}
\begin{enumerate}
\item $\mathbf{0} \in S^\perp$;
\item $\mathbf{w}_1 \in S^\perp, \mathbf{w}_2 \in S^\perp \implies \forall i: (\mathbf{w}_1 + \mathbf{w}_2)\cdot \mathbf{v}_i = 0$ because of the distributive property;
\item $\forall i: (\alpha \mathbf{w}_1) \cdot \mathbf{v}_i = \alpha (\mathbf{w}_1 \cdot \mathbf{v}_i) = 0$.
\end{enumerate}
\end{proof}

In $\mathbb{R}^n$, the perpendicular set of a subspace of dimension $d$ has dimension $n-d$.

\chapter{Spaces}

\section{Affine spaces}

They resolve the ambiguity between vectors and points. $\mathbb{A}^3 (\mathbb{R})$ is affine to $\mathbb{R}^3$.

\begin{definition}
An affine space is the set of points on which the vector space $\mathbb{R}^3$ acts.
\begin{equation}
f: \mathbb{A}^3 (\mathbb{R}) \times \mathbb{R} \rightarrow \mathbb{A}^3 (\mathbb{R})
\end{equation}

So to a couple (point, vector) we associate a vector.
\end{definition}

\paragraph{Properties}

\begin{enumerate}
\item Transitive: $\forall (p, q) \in \mathbb{A}^3 (\mathbb{R}): \exists ! \mathbf{v} \in \mathbb{R}^3$  such that  $q = p+\mathbf{v}$ (or, $\mathbf{v}$ is uniquely determined by $(p, q)$;
\item Compatibility with the algebraic operations in $\mathbf{R}^3$ (sum, neutral element...);
\item If we choose an $O \in \mathbb{A}^3 (\mathbb{R})$ we get $\mathbb{A}^3 (\mathbb{R}) \leftrightarrow \mathbb{R}^3$ (a reference frame in the affine space corresponds to a basis in the vector space);
\end{enumerate}

An affine space has more freedom than a vector space, since we can change the reference point.

\subsection{Linear varieties}

They correspond to the subspaces of the vector space. 

\begin{definition}
They are subsets of $\mathbb{A}^3 (\mathbb{R})$ in the form $P_0 + U = \lbrace Q \in \mathbb{A}^3 (\mathbb{R}) : Q-P_0 \in U\rbrace$, where $P_0$ is a point in space and $U$ is a subspace of $\mathbb{R}^3$.

\begin{equation}
P_0 + U = \lbrace q \in \mathbb{A}^3 (\mathbb{R}): q-P_0 \in U\rbrace
\end{equation}
\end{definition}

If $Q_1$ and $Q_2$ belong to $P_0 + U: Q_i = P_0 + U_i$, where $U_i \in U$, then $Q_2-Q_1 = U_2 - U_1 \in U$ is the vector connecting the points.

\paragraph{Potential linear varieties of $\mathbb{A}^3 (\mathbb{R})$}

\begin{itemize}
\item $P_0 +\lbrace \mathbf{0}\rbrace$ is just $P_0$;
\item $P_0 + \mathbf{R}^3 = \mathbb{A}^3 (\mathbb{R})$, is all of space;
\item if $U = \langle\mathbf{u}\rangle$, with $\mathbf{u}\neq 0$, the dimension of $U$ is 1, and then $P_0 + U = \lbrace P_0 + \lambda \mathbf{u}\rbrace$;
\item if $U = \langle\mathbf{u}_1; \mathbf{u}_2\rangle$, the dimension of $U$ is 2, and $P_0 + U = \lbrace P_0 + \lambda \mathbf{u}_1 + \mu \mathbf{u}_2\rbrace$.
\end{itemize}

The subspace $U$ cannot be changed for another: it is called ``giacitura''.

We can do an analogous operation in $\mathbf{R}^2$.

\subsubsection{Parametric equations of linear varieties}

Describing a linear variety in $\mathbb{A}^3 (\mathbb{R})$ as $P + \langle \mathbf{x}\rangle$ gives us parametric equations.
We just apply the definition; for example, in the case of a linear variety of dimension 1:

\begin{equation}
r: P_1 + \langle P_2 - P_1\rangle = P_1 + \alpha \begin{pmatrix}
x_2 - x_1 \\
y_2 - y_1 \\
z_2 - z_1 
\end{pmatrix}
\end{equation}

\subsubsection{Cartesian equations of linear varieties}

We can find them by taking the orthogonal of the orthogonal subspace, and substituting $(x_i - P_i)$ for each coordinate.

\section{Euclidean spaces}

If we consider the structure ``vector subspace'' equipped with the notion of scalar product, we get an Euclidean space, denoted $\mathbb{E} ^3 (\mathbb{R})$. This also means we have a notion of distance: $\text{d}(P, Q) = \norm{\overrightarrow{PQ}}$, where $\overrightarrow{PQ}$ is $Q-P \in \mathbb{R}^3$.

Given $S, T \subseteq \mathbb{E}^3 (\mathbb{R})$, we define

\begin{equation}
\text{d} (S, T) := \inf \left\lbrace \text{d} (P, Q), P \in S, Q \in T\right\rbrace \subseteq [0, +\infty [
\end{equation}

Note that this does not abide the usual axioms for a distance function: the distance between two different sets can be 0.

\subsection{Distance between linear varieties}

\begin{theorem}
If $S$ and $T$ are linear varieties, $\exists p_0 \in S, q_0 \in T : \text{d} (p_0, q_0) = \text{d} (S, T)$.
\end{theorem}

\begin{proof}
The linear varieties are $S = p_1 + U$ and  $T= q_1 + W$, where $U, W \subseteq \mathbb{R}^3$.

It will be shown later that there exist $\mathbf{u} \in U, \mathbf{w} \in W, \mathbf{n} \in \lbrace U \cup W \rbrace ^\perp$ such that $p_1 q_1 = \mathbf{u} + \mathbf{w} + \mathbf{n}$, but we will use the result straight away.

Take the points $p_0 = p_1 + \mathbf{u}$, $q_0 = q_1 + \mathbf{w}$. Then

\begin{equation}
\text{d}(p_0, q_0) = \norm{q_1 - \mathbf{w} - p_1 - \mathbf{u}} = \mathbf{n}
\end{equation}

Now to show that $(p_0, q_0)$ is minimal: take $(p', q') = (p_0 + \mathbf{u}', q_0 + \mathbf{w}')$.

\begin{equation}
\text{d}(p', q')^2 = \norm{q'-p'}^2 = \norm{q_0 - p_0 + \mathbf{w}' - \mathbf{u}'}^2 = \norm{\mathbf{n} + \mathbf{w}' - \mathbf{u}'}^2 
\end{equation}

but we also know that

\begin{equation}
\mathbf{n} \cdot (\mathbf{w} - \mathbf{u}) = 0
\end{equation}

since $\mathbf{n} \in \lbrace U \cup W \rbrace ^\perp$, so

\begin{equation}
\norm{\mathbf{n} + \mathbf{w}' - \mathbf{u}'}^2 = \norm{\mathbf{n}}^2 + \norm{\mathbf{w} -\mathbf{u}}^2 \geq \norm{\mathbf{n}}
\end{equation}

as desired. We have equality only if $\norm{\mathbf{w} -\mathbf{u}}^2=0$; if $U$ and $W$ have nonzero vectors in commons (i. e. they are parallel), there are $(p', q')$ such that $\text{d}(p', q')=\text{d}(p_0, q_0)$. 
\end{proof}

We can find intersections between linear varieties using cartesian or parametric equations, but it is often best to mix: use the cartesian equation for one variety and the parametric for the other.

\section{Plane bundles}

In $\mathbb{A}^3 (\mathbb{R})$, plane bundles are a useful construct. We start with two planes:

\begin{align}
\pi &: ax + by + cz + d=0\\
\pi ' &: a'x+b'y+c'z+d'=0
\end{align}

Their plane bundle is made as such:

\begin{equation}
\lambda (ax + by + cz + d ) + \mu (a'x+b'y+c'z+d')=0
\end{equation}

for some $\lambda$ and $\mu \in \mathbb{R}$, such that $(\lambda, \mu) \neq (0, 0)$.
We immediately notice that for any $\alpha \in \mathbb{R}^*$, $(\lambda, \mu)$ and $(\alpha \lambda, \alpha \mu)$ correspond to the same plane.

\paragraph{Intersecting planes} If $\pi \cup \pi' = r$, a line, then $P \in r \implies P \in \zeta$ for any $\zeta$ in the bundle, because the coordinates of $P$ solve both of the planes' equations. So this is the \emph{proper bundle} of axis $r$.

\paragraph{Parallel planes} If $\pi: P + \langle \mathbf{v}, \mathbf{u}\rbrace$ and $\pi': P' + \langle \mathbf{v}, \mathbf{u}\rbrace$, then it must be that $a = a'$, $b=b'$ and  $c=c'$, since the perpendicular vector is the same. So the plane equation looks like:

\begin{equation}
(\lambda + \mu ) (ax + by+ cz) + \lambda d + \mu d'=0
\end{equation}

Which is parallel to the two starting planes, so the bundle consists of all the parallel planes to the starting ones.

\subsection{Plane-point distance}

We want to find the distance between the point $P$ and the plane $T = Q + V$. We call the projection of $P$ onto the plane $Q_0$.

The cartesian equation of $T$ is $ax+by+cz+d=0$. Then the perpendicular subspace $V^\perp$ is

\begin{equation}
V^\perp = \langle \begin{pmatrix}
a\\b\\c
\end{pmatrix}\rangle = \langle \mathbf{n}\rangle
\end{equation}

$P_0 Q_0$ is proportional to $\mathbf{n}$, $QQ_0$ is perpendicular to it. Now, interpret $QP$ as $Q Q_0 + Q_0 P$ and take $QP \cdot \mathbf{n} = QQ_0 \cdot \mathbf{n} + Q_0 P \cdot \mathbf{n} = \alpha \norm{\mathbf{n}}^2$. Since $\mathbf{n}$ is arbitrary, we choose $\norm{\mathbf{n}}=1$, so $\norm{Q_0 P} = \abs{\alpha}$, so we can calculate $\alpha$ with the scalar product $QP \cdot \mathbf{n}$, without knowing anything about $Q_0$.

With our choice of $\mathbf{n}$, the perpendicular subspace became

\begin{equation}
V^\perp = \left\langle
\begin{pmatrix}
a\\b\\c
\end{pmatrix}
\frac{
1}
{
\sqrt{a^2 + b^2 + c^2}
}
\right\rangle
\end{equation}

Now we calculate

\begin{equation}
\text{d}(P, T) = \abs{\alpha} =
\abs{
QP \cdot \begin{pmatrix}
a\\b\\c
\end{pmatrix}
\frac{1
}{
\sqrt{a^2 + b^2 + c^2}
}}
\end{equation}

The vector $QP$ is

\begin{equation}
\begin{pmatrix}
x_1 - x_0\\
y_1 - y_0\\
z_1 - z_0
\end{pmatrix}
\end{equation}

So

\begin{equation}
\text{d}(P, T) = \abs{
\frac{
ax_0 + by_0 + c z_0 -a x_1 - b x_1 - c x_1
}{
\sqrt{a^2 + b^2 + c^2}
}
}
\end{equation}

But $Q \in T$, so $a x_1 + b x_1 + c x_1 + d = 0$, so $d = -a x_1 - b x_1 - c x_1$.

\begin{equation}
\text{d}(P, T) = \abs{
\frac{
ax_0 + by_0 + c z_0 +d
}{
\sqrt{a^2 + b^2 + c^2}
}
}
\end{equation}

which is the final formula.

\subsection{Plane-plane or plane-line distance}

If they intersect, the distance is 0. Otherwise, we just pick a point from one plane, and then calculate its distance from the other, since they are parallel. The same holds for a plane and a line.

\subsection{Point-line distance}

If $P \in r$, the distance is 0. Otherwise, $\text{d} (P, r) = \text{d} (P, Q_0)$, where $Q_0$ is the projection of $P$ onto the line. If we take a plane $\sigma$ such that $\sigma \perp r$ and $P\in \sigma$, then $Q_0 = \sigma \cap r$.

If we do not care about the position of $Q_0$, we can just take any point $Q \in r$ and $\mathbf{v}\in r$, and calculate:

\begin{equation}
\text{d} (P, r) = \frac{\norm{\overrightarrow{QP} \times \mathbf{v}}}{\norm{v}}
\end{equation}

which is the height of the parallelogram spanned by $\mathbf{v}$ and $\overrightarrow{QP}$.

\subsection{Line-line distance}

If $r \cap s \neq \emptyset$, the distance is 0. If $r \parallel s$, we use the point-line formula. 

\paragraph{Mixed product method}
Otherwise, we can choose $P \in r$ and $Q \in s$, $\mathbf{w}$ in the direction of $r$ and $\mathbf{v}$ in the direction of $s$. Now, we take the volume of the parallelepiped spanned by $\overrightarrow{QP}$, $\mathbf{w}$ and $\mathbf{v}$ and divide it by the area of its base:

\begin{equation}
h = \frac{\abs{\overrightarrow{QP} \cdot (\mathbf{v}\times \mathbf{w})}}{\norm{\mathbf{v}\times \mathbf{w}}}
\end{equation}

\paragraph{Plane method}
Take a plane $\sigma$ such that $s \in \sigma$ and $\sigma \parallel r$. Then, $\text{d} (r, s) =\text{d}(r, \sigma) =  \text{d}(P\in r, \sigma)$

\paragraph{Linear variety method}
We can write $r = P + \langle \mathbf{w}\rangle$ and $s = Q + \langle \mathbf{v} \rangle$. We must choose our $P$ and $Q$ such that $\overrightarrow{PQ} \cdot \mathbf{v} = \overrightarrow{PQ} \cdot \mathbf{w} = 0$. Then $\norm{\overrightarrow{PQ}}$ is the distance.

\section{Abstract vector spaces}

\begin{definition}
A vector space over a field $\mathbb{F}$ is a set $V \neq \emptyset$ equipped with:

\begin{itemize}
\item addition: $v_1, v_2 \in V \implies v_1 + v_2 \in V$;
\item scalar multiplication: $\alpha \in \mathbb{F}, v \in V \implies \alpha v \in V$;
\end{itemize}

which must satisfy, $\forall a, b, c \in V$ and $ \forall \alpha, \beta \in \mathbb{F}$:

\begin{itemize}
\item commutativity for addition: $a+ b = b+a$;
\item associativity for addition:  $(a+b) +c = a + (b+c)$;
\item existence of a neutral element for addition $\exists \mathbf{0} \in V: a + \mathbf{0} = a$;
\item existence of additive inverses: $\exists v\in V: v+a = \mathbf{0}$;
\item distributivity of scalar multiplication with respect to addition: $\alpha (a+b) = \alpha a + \alpha b$;
\item distrivutivity of scalar addition with respect to scalar multiplication: $(\alpha + \beta) a = \alpha a + \beta a$;
\item associativity of scalar multiplication: $\alpha (\beta a) = (\alpha \beta ) a$;
\item identity element of scalar multiplication: $\exists 1 \in \mathbb{F}: 1a = a$.
\end{itemize}
\end{definition}

\subsection{Examples}

$\mathbb{R}^n$ and the geometric vectors are real vector spaces. $\mathbb{R}[x]$ are the polynomials with real coefficients; addition and scalar multiplication are defined as usual.

If we take $I \subseteq \mathbb{R}$, the set of all $f: I \rightarrow \mathbb{R}$ is a vector space. Addition and scalar multiplication can be defined point by point on the image. Subspaces of this vector space are the continuous functions and the differentiable functions.

Real or complex matrices with $n$ rows and $m$ columns are a vector space (denoted by $M_{n\times m} (\mathbb{R})$). Addition and scalar multiplication are defined as: $a_{ij} + b_{ij} = c_{ij}$, $\alpha a_{ij} = c_{ij}$.

\section{Subspaces}

$U\subseteq V$, $U\neq \emptyset$ is a vector subspace if the operations of $V$, restricted to $U$, make it a vector space. $U$ is subspace iff it contains all the linear combinations of its vectors.

\paragraph{Generators and bases}

\begin{definition}
Given the subspace $U$, the vectors $\mathbf{u}_i$ are \emph{generators} if every vector in $U$ can be written as a linear combination of them.
\end{definition}

\section{Bases}

\begin{definition}
A basis of a subspace $U$ is a set of linearly independent generators.
\end{definition}

For example, the canonical basis of $M_{n\times m} (\mathbb{R})$ is $\mathbf{e}_{ij} = \delta_{ij}$.\footnote{$\delta_{ij}$ is the Dirac delta, which is 1 only if the two indices are equal, and 0 otherwise.}

\begin{theorem}
Given a vector subspace $U\subseteq V$, the vectors $\mathbf{u}_i \in U$ are a basis iff every vector in $U$ can be written in a unique way as a linear combination of $\mathbf{u}_i$.
\end{theorem}

\begin{proof}
Leftward implication: if every vector can be written in a unique way, $\mathbf{u}_i$ are generators, and $\mathbf{0} = \sum 0 \mathbf{u}_i$: by the fact that the representation is unique $\mathbf{u}_i$ are linearly independent.

Rightward implication: if $\mathbf{u}_i$ is a basis, take $\mathbf{v} = \sum \alpha_i \mathbf{u}_i = \sum \beta_i \mathbf{u}_i$. Then $\mathbf{v} - \mathbf{v} = \mathbf{0} = \sum (\alpha_i - \beta_i ) \mathbf{u}_i$, which implies that $\forall i \in \mathbb{N} \alpha_i = \beta_i$, that is, the representation is unique.
\end{proof}

A set of vectors $S$ can be a set of generators even if it is infinite, as long as any vector is written as a combination of just a finite subset of them. For example, the set $S = \lbrace x^n | \forall n \in \mathbb{N} \rbrace$ is a set of generators for the polynomials.

\paragraph{Elimination}

\begin{theorem}
If $\mathbf{u}_i, i \in [1, n]$ are linearly dependent generators of $U$, then $\exists k \in [1, n]: \mathbf{u}_i, i \in [1, n]\smallsetminus \lbrace k\rbrace$ are generators of $U$.
\end{theorem}

\begin{proof}
In the combination $\mathbf{0} = \sum \alpha_i \mathbf{u}_i$ at least one $\alpha_i$ must be nonzero. Take one such coefficient and divide by it:

\begin{equation}
\frac{\alpha_i \mathbf{u}_i}{\alpha_i} = -\frac{1}{\alpha_i} \displaystyle\sum_{\mathclap{\substack{k=1 \\ k\neq i}}}^n \alpha_k \mathbf{u}_k
\end{equation}

So we have written $\mathbf{u}_i$ as a linear combination of the other basis vectors. Now, we can express any vector in $U$ without it.
\end{proof}

In a finitely generated space (a space with finitely many generators), we can always find a basis applying this theorem repeatedly.

\begin{theorem}
If $\mathbf{u}_i$ are linearly independent vectors in a subspace $U$, given a vector $\mathbf{v} \in U$, the vectors $\lbrace \mathbf{u}_i, \mathbf{v}\rbrace$ are linearly independent iff $\mathbf{v} \notin \langle \mathbf{u}_i \rangle$.
\end{theorem}

\begin{proof}
Leftward implication: if $\mathbf{v} \in \langle \mathbf{u}_i \rangle$, then $\mathbf{v}- \sum \alpha_i \mathbf{u}_i = \mathbf{0}$, so they are linearly dependent.

Rightward implication: if there exist coefficients $\lbrace \beta, \alpha_i \rbrace$ such that $\beta \mathbf{v} + \sum \alpha_i \mathbf{u}_i = 0$, then $\beta \neq 0$ since $\mathbf{u}_i$ are linearly independent, then we can divide by $\beta$ and:

\begin{equation}
\mathbf{v} = -\frac 1\beta \sum_i \alpha_i \in \langle \mathbf{u}_i\rangle
\end{equation}
\end{proof}

\paragraph{Constructing a basis}

Given a finitely generated subspace $U\neq \mathbf{0}$, we can construct a basis for it: choose a vector $\mathbf{u}_1$, which is independent. If $U = \langle \mathbf{u}_1\rangle$, we are done. If not, $\exists \mathbf{u}_2 \in U, \mathbf{u}_2 \notin \langle \mathbf{u}_1\rangle$, so they are independent. We can go on as long as the generated subspace does not equal $U$.

\begin{theorem}[Steinitz exchange lemma]
Given the linearly independent vectors $\lbrace \mathbf{u}_i\rbrace_{i \in [1, k]} \in V$ and the vectors $\langle \mathbf{v}_i\rangle_{i \in [1, p]} = V$, then $k \leq p$.
\end{theorem}

\begin{proof}
$\mathbf{u}_1 = \sum \alpha_i \mathbf{v}_i$. $\forall i: \mathbf{u}_i \neq \mathbf{0}$, so at least one $\alpha_i$ is not 0. WLOG, we can take $\alpha_1 \neq 0$. Now,

\begin{equation}
\mathbf{v}_1 = \frac{1}{\alpha_1} \mathbf{u}_1 - \sum_{i=2}^p \frac{\alpha_i \mathbf{v}_i}{\alpha_1}
\end{equation}

so $\langle \mathbf{u}_1, \mathbf{v}_i \rangle =V$. We have substituted one of the generators: we wish to substitute $k$ generators, but we must watch out for an issue: in

\begin{equation}
\mathbf{u}_2 = \beta_1 \mathbf{u}_1 \sum_{i=2}^p \beta_i \mathbf{v}_i
\end{equation} 

one of the $\beta_i$ must be $\neq 0$, but the proof would not work if it were just $\beta_1$.

This is easily fixed: if $\forall i \in [2, p]: \beta_i =0$, then we would have $\mathbf{u}_2 = \beta_1 \mathbf{u}_1$, which is excluded since they are independent. So we suppose, again WLOG, that $\beta_2 \neq 0$, and we can continue:

\begin{equation}
\mathbf{v}_2 = -\frac 1{\beta_2}\left( \beta_1 \mathbf{u}_1 - \mathbf{u}_2 + \sum_{i=3}^p \beta_i \mathbf{v}_i \right)
\end{equation}

This way, we can substitute $k$ of the $p$ generators and we will still have generators. So, $k\leq p$.
\end{proof}

\begin{corollary}
All the bases of a subspace have the same number of elements, we call this the \emph{dimension} of the subspace.
\end{corollary}

\begin{proof}
Given $n$ vectors in a basis and $k$ in another basis, we use alternatively the fact that they are generators and independent to see that $k\leq n$ and $n\leq k$.
\end{proof}

\begin{corollary}
If $U\subseteq W \subseteq V$ and $U, W$ are subspaces, then if $\dim U = \dim W$ then $U=W$.
\end{corollary}

\begin{proof}
Given the basis vectors of $U$, $\lbrace \mathbf{u}_i\rbrace _{i\in [1, n]}$, $\lbrace \mathbf{u}_i, \mathbf{w}\rbrace$ are linearly independent iff $\mathbf{w} \notin U$, so if $\exists \mathbf{w} \in W: \mathbf{w}\notin U$, then there would be $n+1$ linearly independent vectors in $W$, which is impossible, so $\forall\mathbf{w} \in W, \mathbf{w} \in U$.
\end{proof}

\section{Operations on subspaces}

\subsection{Intersection}

\begin{theorem}
Given a vector space $V$ and the subspaces $W \subseteq V$ and $U\subseteq V$, then $W\cap U$ is a subspace.
\end{theorem}
\begin{proof}
$\mathbf{0} \in W\cap U$, and given $\mathbf{v}_1, \mathbf{v}_2 \in W\cap U$, $\mathbf{v}_1 + \mathbf{v}_2 \in U$ and $\mathbf{v}_1 + \mathbf{v}_2 \in W$, so $\mathbf{v}_1 + \mathbf{v}_2 \in W\cap U$. The same holds for scalar multiplication.
\end{proof}

\subsection{Sum}

The union of subspaces is generally not a subspace, and we only need one example to show this. So, we define:

\begin{definition}
Given the subspaces $U$ and $W$, their \emph{sum space} is defined as $U+W := \langle U \cup W \rangle$.\footnote{Obviously the set of genenerators given for this subspace are not a basis.} This is the smallest subspace containing both $U $ and $W$.
\end{definition}

\begin{definition}
We say that the sum of two subspaces is \emph{direct} iff a vector in it is written uniquely as a linear combination of vectors of each starting subspace; this is denoted like: $S \oplus T$.
\end{definition}

For example, $\mathbb{R}^2 = \langle \mathbf{e}_1 \rangle \oplus \langle \mathbf{e}_2 \rangle$.

\begin{theorem}
The sum of two subspaces is direct iff their intersection is exactly $\mathbf{0}$.
\end{theorem}

\begin{proof}
Leftward implication: given $\mathbf{v} \in S \oplus T$, take $\mathbf{v} \in S\cap T$. Then $\mathbf{v} = (\mathbf{v}_S + \mathbf{0}_T) = (\mathbf{0}_S + \mathbf{v}_T)$. By the uniqueness of the representation, $\mathbf{v} = \mathbf{0}$.

Rightward implication: given that $S \cap T = \lbrace \mathbf{0}\rbrace$, take $\mathbf{v} \in S+T$, and suppose that $\mathbf{v} = \mathbf{s}_1 + \mathbf{t}_1 = \mathbf{s}_2 + \mathbf{t}_2$.

Now, $\mathbf{s}_1 - \mathbf{s}_2 \in S$ and $\mathbf{t}_1 - \mathbf{t}_2 \in T$, but $\mathbf{s}_1 - \mathbf{s}_2 = \mathbf{t}_1 - \mathbf{t}_2$ so they belong to $S \cap T$, which implies $\mathbf{s}_1 - \mathbf{s}_2 = \mathbf{t}_1 - \mathbf{t}_2 = \mathbf{0}$.
\end{proof}

It is clear that
\begin{equation}
\dim U + \dim W \geq \dim (U+W) \geq \max \lbrace \dim U , \dim W \rbrace
\end{equation}

\begin{theorem}[Grassmann's formula]
\begin{equation}
\dim (S+T) = \dim S + \dim T - \dim (S\cap T)
\end{equation}
\end{theorem}

A sketch of the proof is as follows:

We construct a basis for $S \cap T$, then we add vectors until we get to a basis of $S$, and we do the same for $T$. All these vectors must then be a basis for $S+T$ (it must be proven that they are independent generators), but we counted the ones from $S\cap T$ twice. 

\chapter{Matrices}

\section{Elementary operations}

Given the set of generators $U = \langle \mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\rangle$, we can perform the following operations without changing $U$:

\begin{enumerate}
\item switching places: $U = \langle \mathbf{u}_2, \mathbf{u}_1, \dots, \mathbf{u}_2 \rangle$;
\item adding to any generator any other generator scaled by some amount: $U = \langle (\mathbf{u}_1 + \alpha \mathbf{u}_2), \mathbf{u}_2, \dots, \mathbf{u}_n \rangle$
\item scaling any generator: $U = \langle \alpha \mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\rangle$
\end{enumerate}

We can perform these to the rows of a matrix.

\begin{definition}
A matrix is said to be in \emph{row echelon form} if:
\begin{enumerate}
\item all the rows composed by just zeroes are on the bottom of the matrix;
\item the pivot\footnote{The first nonzero coefficient in a row of a matrix is called \emph{pivot}.} of any row is strictly to the right of the one of the row above it.
\end{enumerate}
\end{definition}

\begin{definition}
The \emph{rank by rows} of a matrix is the dimension of the space spanned by its rows. 
\end{definition}

If a matrix is in row echelon form, this is the number of nonzero rows. The rank of a matrix is invariant under elementary operations.

\section{Matrix multiplication}

This is the first operation we introduce that distinguishes matrices from any other vector space. 

If we multiply $\mathbf{A} \in M_{m\times n} (\mathbb{F})$ and $\mathbf{B} \in M_{p\times n} (\mathbb{F})$ we get $\mathbf{AB} = \mathbf{C}$ such that:

\begin{equation}
\mathbf{C}_{ij} = \sum_{k=1}^m \mathbf{A}_{ik} \mathbf{B}_{kj}
\end{equation}

It has all the properties we would expect, with respect to both addition and scalar multiplication: associativity, distributivity, but it is not commutative. The identity matrix $I_n \in M_{n\times n}(\mathbb{F})$ is defined so that $I_{ij} = \delta_{ij}$, and its columns (or rows) are the canonical basis of $\mathbb{R}^n$.

\subsection{Elementary operations through matrix multiplication}

We can express all the elementary operations through multiplying to the left by some matrix, which can be obtained by performing the operations on the identity matrix.

\subsection{Linear applications}

An application is linear if it preserves the operations on the spaces:

\begin{align}
f: V\rightarrow W \text{ linear} \iff \forall \mathbf{v}_1, \mathbf{v}_2 \in V: f(\mathbf{v}_1) + f(\mathbf{v}_2) &= f(\mathbf{v}_1 + \mathbf{v}_2) \\
\forall \alpha \in \mathbb{R}: f(\alpha \mathbf{v}_1) &= \alpha f(\mathbf{v}_1)
\end{align}

A linear function $f: \mathbb{R}\rightarrow \mathbb{R}$ must satisfy $f(x) = f( 1x) = x f(1)$, so it is necessarily of the form $f(x) = \alpha x$ for some $\alpha\in\mathbb{R}$.

By the same reasoning, a function $f: \mathbb{R}^n\rightarrow \mathbb{R}^m$ is uniquely determined by the vectors onto which the basis vectors land, and is described by a matrix $\mathbf{A} \in M_{m\times n}(\mathbb{R})$: $f(\mathbf{x}) = \mathbf{Ax}$. The columns of $\mathbf{A}$ are the vectors onto which the basis vectors land.

\begin{theorem}
Given a linear function $f: V \rightarrow W$, it preserves subspaces:
\begin{enumerate}
\item if $S\subseteq V$ is a subspace, $f(S)$ is a subspace in $W$;
\item if $T\subseteq W$ is a subspace, $f^{-1} (T) = \lbrace \mathbf{v} \in V: f(\mathbf{v}) \in T\rbrace$ is a subspace in $V$.
\end{enumerate}
\end{theorem}

\begin{proof}
The proof follows trivially from the properties of linear functions.
\end{proof}

We find that the image of the function $f(V)$ is a subspace of $W$.

\begin{definition}
\begin{equation}
\ker f = f^{-1} (\mathbf{0}) = \lbrace \mathbf{v} \in V : f(\mathbf{v}) = \mathbf{0}\rbrace
\end{equation}
\end{definition}

\begin{theorem}
A linear function $f: V \rightarrow W$ is surjective iff $f(V) = W$ and injective iff $\ker f = \lbrace \mathbf{0}\rbrace$. \footnote{Note that this is only the case for \emph{linear} functions.}
\end{theorem}

\begin{proof}
The surjective part is trivial.

Leftward implication: if $f$ is injective,  $\mathbf{v} \in \ker f$, $\mathbf{v} = f(\mathbf{0}) \implies \mathbf{v} = \mathbf{v} = \mathbf{0}$ since $f(\mathbf{0}) = \mathbf{0}$.

Rightward implication: if $\ker f = \lbrace \mathbf{0}\rbrace$, take $\mathbf{v}_1, \mathbf{v}_2 \in V$ such that $f(\mathbf{v}_1) = f(\mathbf{v}_2)$. Then

\begin{equation}
f(\mathbf{v}_1) - f(\mathbf{v}_2) = f(\mathbf{v}_1 - \mathbf{v}_2) = \mathbf{0} \implies
\mathbf{v}_1 - \mathbf{v}_2 \in \ker f \implies \mathbf{v}_1 - \mathbf{v}_2 = \mathbf{0}
\end{equation}
\end{proof}

If $V = \langle \mathbf{v}_i \rangle$, and $f: V \rightarrow W $ is linear, then $f(V) = \langle f(\mathbf{v}_i) \rangle$.

So $\dim (f(V) ) \leq \dim V$, since there will be $\dim V$ generators.

\subsection{Dimensions}

\begin{theorem}[Dimension theorem]
Given a linear function $f: V\rightarrow W$, if $V$ is finitely generated, then

\begin{equation}
\dim V = \dim f(V) + \dim \ker f
\end{equation}
\end{theorem}

\begin{proof}
We construct a basis for $V$ from a basis of $\ker f$ and one of $f(V)$.

If $\lbrace \mathbf{u}_i \rbrace$ is a basis for $\ker f$ and $\lbrace\mathbf{w}_i \rbrace $ is a basis for $f(V)$, we take the vectors $\lbrace \mathbf{v}_i \rbrace$ such that $\forall i: f(\mathbf{v}_i) = \mathbf{w}_i$. Then $\lbrace \mathbf{v}_i , \mathbf{u}_i \rbrace$ is a basis for $V$:


\textbf{Generators}: take any $\mathbf{x} \in V$, then $f(\mathbf{x}) \in f(V)$, so $f(\mathbf{x}) = \sum \beta_i \mathbf{w}_i$. Therefore, $f(\mathbf{x} - \sum \beta_i \mathbf{v} ) = f(\mathbf{x}) - \sum \beta_i f(\mathbf{v}_i) = \mathbf{0}$.

So, $\mathbf{x} - \sum \beta_i \mathbf{v}_i \in \ker f \implies \mathbf{x} - \sum \beta_i \mathbf{v}_i = \sum \alpha_i \mathbf{u}_i$, which gives us the final formula:

\begin{equation}
\mathbf{x} = \sum_i \beta_i \mathbf{v}_i + \sum_i \alpha_i \mathbf{u}_i
\end{equation}

\textbf{Independent}: we take

\begin{equation}
\sum_i \beta_i \mathbf{v}_i + \sum_i \alpha_i \mathbf{u}_i = \mathbf{0}
\end{equation}

and apply $f$: the $\alpha$s must go to 0 since their combination is in the kernel, and the $\beta$s must go to 0 since the basis vectors for $V$ are independent.
\end{proof}

\section{Linear applications through matrices}

Given a matrix $\mathbf{A} \in M_{m\times n} (\mathbb{R})$, we define $L_A: \mathbb{R}^m \rightarrow \mathbb{R}^n$ as: $\mathbf{x} \rightarrow \mathbf{Ax}$. The image of $L_A$ is the subspace generatd by the columns of $\mathbf{A}$.

\begin{definition}
The rank by columns of $\mathbf{A}$ is the dimension of the subspace generated by its columns, and the same goes for the rows: $\rank_C A = \dim \im(L_A)$.
\end{definition}

The kernel of $L_A$ is just the set of solutions to $\mathbf{Ax} = \mathbf{0}$.

\begin{theorem}
\begin{equation}
\rank \mathbf{A} = \rank \mathbf{A}^T
\end{equation}
\end{theorem}

\begin{proof}
%Suppose WLOG that $\mathbf{A}$ is in row echelon form. Then $\rank_C \mathbf{A} = \rank _R \mathbf{A}$ is the number of pivots.

??? Abate-DF pag. 101
\end{proof}

The columns containing the pivots also form a basis for the column space of $\mathbf{A}$.

\paragraph{Inverse images}

What does the set $f^{-1} (\mathbf{w})$ look like? If it is not the kernel it is not a subspace, but, if $f^{-1} (\mathbf{w}) \neq \emptyset$, we can take two vectors $\mathbf{v}_1, \mathbf{v}_2 \in f^{-1} (\mathbf{w})$, and write:

\begin{equation}
\mathbf{0} = \mathbf{w} - \mathbf{w} = f(\mathbf{v}_1) - f(\mathbf{v}_2) \implies \mathbf{v}_1 - \mathbf{v}_2 \in \ker f
\end{equation}

So, all the sets of counterimages of vectors can be written as $f^{-1} (\mathbf{w}) = \lbrace \mathbf{u} + \mathbf{v}, \forall \mathbf{v} \in\ker f \rbrace$ and are thus linear varieties of direction.

\section{Linear systems}

Given the matrix $\mathbf{A}_{m\times n} (\mathbb{R})$, we define the function $L_A$ as usual. Now, for some $\mathbf{b} \in \mathbb{R}^n$, $L_A^{-1} (\mathbf{b}) \neq \emptyset \iff \mathbf{b} \in \im L_A \iff \im L_A = \langle \mathbf{A}_i, \mathbf{b}\rangle$ where $\mathbf{A}_i$ are the columns of the matrix.

So, we can write the matrix $(\mathbf{A}|\mathbf{b}) \in M_{m\times(n+1)} (\mathbb{R})$ which is $\mathbf{A}$ with an additional column containing $\mathbf{b}$.

\begin{theorem}[RonchÃ© --- Capelli]
The linear system $\mathbf{Ax} = \mathbf{b}$ has solution iff $\rank \mathbf{A} = \rank (\mathbf{A}|\mathbf{b})$. If it does, the solutions form a linear variety with $\ker L_A$ as its directing space; therefore it has dimension $n - \rank A$.
\end{theorem}

\tableofcontents

\end{document}
